{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# answers to holidays\n",
    "docs = [\n",
    "    'I went to France to visit Paris',\n",
    "    'New York City was crowded',\n",
    "    'sightseeing in Bali',\n",
    "    'swimming in the tropical oceans',\n",
    "    'swimming in Bali',\n",
    "    'raining in Paris',\n",
    "    'visiting monuments in Paris',\n",
    "    'San Francisco',\n",
    "    'San Francisco',\n",
    "    'flew to Berlin',\n",
    "    'enjoyed visiting eiffel tower in paris',\n",
    "    'Paris, city of love',\n",
    "    'Flying is fun',\n",
    "    'Paris is crowded',\n",
    "    'France'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped docs [went France visit Paris, New York City crowded, sightseeing Bali, swimming tropical oceans, swimming Bali, raining Paris, visiting monuments Paris, San Francisco, San Francisco, flew Berlin, enjoyed visiting eiffel tower paris, Paris city love, Flying fun, Paris crowded, France]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "def stop_word_stripper(line):\n",
    "    stop_words = [w.strip('\\n').lower() for w in open('stop_words.txt').readlines()]\n",
    "    pos_stopper = ['PUNCT', 'SYM']\n",
    "    return ' '.join([token.text for token in line if str(token).lower() not in stop_words and token.pos_  not in pos_stopper])\n",
    "\n",
    "stripped_docs = [] #spacy object\n",
    "copy_docs = [] # strings\n",
    "for d in docs:\n",
    "    stripped_docs.append(nlp(stop_word_stripper(nlp(d))))\n",
    "    copy_docs.append(stop_word_stripper(nlp(d)))\n",
    "    \n",
    "print('stripped docs', stripped_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 5, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1} ['went  visit', 'crowded', 'sightseeing', 'swimming tropical oceans', 'swimming', 'raining', 'visiting monuments', '', '', 'flew', 'enjoyed visiting eiffel tower paris', 'city love', 'Flying fun', 'crowded', '']\n"
     ]
    }
   ],
   "source": [
    "# parse through to get entities \n",
    "kw_freq = {}\n",
    "\n",
    "for i in range(len(stripped_docs)):\n",
    "    line = stripped_docs[i]\n",
    "    for e in line.ents:\n",
    "        copy_docs[i] = copy_docs[i].replace(e.text, '').strip()\n",
    "        if e.text in kw_freq:\n",
    "            kw_freq[e.text] += 1\n",
    "        else:\n",
    "            kw_freq[e.text] = 1\n",
    "\n",
    "print(kw_freq, copy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 5, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1, 'visit': 3, 'crowded': 1, 'sightseeing': 1, 'swim': 2, 'tropical': 1, 'ocean': 1, 'rain': 1, 'monument': 1, 'enjoy': 1, 'eiffel': 1, 'tower': 1, 'paris': 1, 'city': 1, 'love': 1, 'flying': 1, 'crowd': 1}\n"
     ]
    }
   ],
   "source": [
    "# get lemma keywords \n",
    "# join the rest of the words together: \n",
    "\n",
    "corpus = nlp(' '.join(copy_docs))\n",
    "\n",
    "MIN_CHARACTERS = 3\n",
    "\n",
    "for token in corpus:\n",
    "    if len(token.lemma_) > MIN_CHARACTERS:\n",
    "        if token.lemma_ in kw_freq:\n",
    "            kw_freq[token.lemma_] += 1\n",
    "        else:\n",
    "            kw_freq[token.lemma_] = 1\n",
    "\n",
    "print(kw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 6, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1, 'visit': 3, 'crowded': 1, 'sightseeing': 1, 'swim': 2, 'tropical': 1, 'ocean': 1, 'rain': 1, 'monument': 1, 'enjoy': 1, 'eiffel': 1, 'tower': 1, 'city': 1, 'love': 1, 'flying': 1, 'crowd': 1}\n"
     ]
    }
   ],
   "source": [
    "# proper casing\n",
    "\n",
    "caseless_freq = {}\n",
    "propercase_freq = {}\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if kw in caseless_freq:\n",
    "        caseless_freq[kw.lower()].append(count)\n",
    "    else:\n",
    "        caseless_freq[kw.lower()] = [count]\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if count == max(caseless_freq[kw.lower()]):\n",
    "        propercase_freq[kw] = sum(caseless_freq[kw.lower()])\n",
    "\n",
    "print(propercase_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 300) ['France', 'Paris', 'New York City', 'Bali', 'San Francisco', 'Berlin', 'visit', 'crowded', 'sightseeing', 'swim', 'tropical', 'ocean', 'rain', 'monument', 'enjoy', 'eiffel', 'tower', 'city', 'love', 'flying', 'crowd']\n"
     ]
    }
   ],
   "source": [
    "# semantic k means clustering\n",
    "\n",
    "glove_vectors = []\n",
    "labels_array = []\n",
    "\n",
    "for kw, count in propercase_freq.items():\n",
    "    labels_array.append(kw)\n",
    "    glove_vectors.append(nlp(kw)[0].vector)\n",
    "\n",
    "print(np.array(glove_vectors).shape, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=5, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k means clustering \n",
    "\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=len(labels_array)//5, n_init=5)\n",
    "kmeans_model.fit(glove_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0 1 1 1 0 0 0 2 2 2 2 3 0 3 3 0 0 3 0]\n",
      "{1: ['France', 'Paris', 'Bali', 'San Francisco', 'Berlin'], 0: ['New York City', 'visit', 'crowded', 'sightseeing', 'enjoy', 'city', 'love', 'crowd'], 2: ['swim', 'tropical', 'ocean', 'rain'], 3: ['monument', 'eiffel', 'tower', 'flying']}\n"
     ]
    }
   ],
   "source": [
    "cluster_labels    = kmeans_model.labels_\n",
    "cluster_inertia   = kmeans_model.inertia_\n",
    "\n",
    "print(cluster_labels)\n",
    "\n",
    "clusters = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "\n",
    "print (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance matrix (len(cluster_labels)^2)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "n = len(labels_array)\n",
    "\n",
    "similarity_matrix = np.zeros([n, n])\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        distance_matrix[i][j] = spatial.distance.cosine(glove_vectors[i], glove_vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bali': 22,\n",
       " 'Berlin': 18,\n",
       " 'France': 22,\n",
       " 'New York City': 18,\n",
       " 'Paris': 40,\n",
       " 'San Francisco': 22,\n",
       " 'city': 18,\n",
       " 'crowd': 18,\n",
       " 'crowded': 18,\n",
       " 'eiffel': 18,\n",
       " 'enjoy': 18,\n",
       " 'flying': 18,\n",
       " 'love': 18,\n",
       " 'monument': 18,\n",
       " 'ocean': 18,\n",
       " 'rain': 18,\n",
       " 'sightseeing': 18,\n",
       " 'swim': 22,\n",
       " 'tower': 18,\n",
       " 'tropical': 18,\n",
       " 'visit': 26}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assign max font size\n",
    "\n",
    "def assign_font_size(propercase_freq, max_size, min_size):\n",
    "    label_fs = {}\n",
    "    sorted_tuples = [(k, propercase_freq[k]) for k in sorted(propercase_freq, key=propercase_freq.get, reverse=True)]\n",
    "    min_count = sorted_tuples[-1][1]\n",
    "    max_count = sorted_tuples[0][1]\n",
    "    \n",
    "    for kw, count in sorted_tuples:\n",
    "        size = int((max_size - min_size)/(max_count - min_count)*count + min_size - (max_size - min_size)/(max_count - min_count)*min_count)\n",
    "        label_fs[kw] = size\n",
    "    \n",
    "    return (label_fs)\n",
    "        \n",
    "assign_font_size(propercase_freq, 40, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aspect ration of words : font_size (length) font_size*0.7(width)\n",
    "# Aspect ration of SVG file is 16:9\n",
    "\n",
    "# How to draw V1 \n",
    "# Create a polygon with the number of vertices = number of clusters \n",
    "# here cluster size = 3 so a triangle (not ever going to exceed 5)\n",
    "# 3 rectangles to fit within the first rectangle \n",
    "\n",
    "# in a 16:9\n",
    "\n",
    "# Cluster one in rect 1 (y = 16, 9/4) (w: 8, l: 9/2)\n",
    "# cluster Two in rect 2 (y = 16, 9/4*3) (w: 8, l: 9/2)\n",
    "# Cluster three in rect 3 (y = 8, 9/4) Biggest cluster ? (w: 8, l: 9/2)\n",
    "\n",
    "# Where to put the words \n",
    "# Start with the highest frequence with the biggest font : assign max font size before starting to draw\n",
    "# If the next one is smaller in frequence, font size drops by \n",
    "# font size 35 to 18\n",
    "# random choice where the word fits \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
