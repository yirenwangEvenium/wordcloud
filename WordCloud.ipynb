{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from math import cos\n",
    "from math import sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [x.strip('\\n') for x in open('data/real6.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped docs [Value, , Feature Set Services, , Value, Service, Red Color, Msp, Protection, Support, Support, Value, Development, Total Security, Total security, Simple, Value, Useful, Performance, Value, Innovation, Intigration, Bundle, Total Security, Support, Totalsecurity, Integration, Simple, Features, Value, Value, Integration]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "def stop_word_stripper(line):\n",
    "    stop_words = [w.strip('\\n').lower() for w in open('data/stop_words.txt').readlines()]\n",
    "    pos_stopper = ['PUNCT', 'SYM']\n",
    "    return ' '.join([token.text for token in line if str(token).lower() not in stop_words and token.pos_  not in pos_stopper])\n",
    "\n",
    "stripped_docs = [] #spacy object\n",
    "copy_docs = [] # strings\n",
    "for d in docs:\n",
    "    stripped_docs.append(nlp(stop_word_stripper(nlp(d))))\n",
    "    copy_docs.append(stop_word_stripper(nlp(d)))\n",
    "    \n",
    "print('stripped docs', stripped_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature Set Services': 1, 'Red Color': 1, 'Msp': 1} ['Value', '', '', '', 'Value', 'Service', '', '', 'Protection', 'Support', 'Support', 'Value', 'Development', 'Total Security', 'Total security', 'Simple', 'Value', 'Useful', 'Performance', 'Value', 'Innovation', 'Intigration', 'Bundle', 'Total Security', 'Support', 'Totalsecurity', 'Integration', 'Simple', 'Features', 'Value', 'Value', 'Integration']\n"
     ]
    }
   ],
   "source": [
    "# parse through to get entities \n",
    "kw_freq = {}\n",
    "\n",
    "for i in range(len(stripped_docs)):\n",
    "    line = stripped_docs[i]\n",
    "    for e in line.ents:\n",
    "        copy_docs[i] = copy_docs[i].replace(e.text, '').strip()\n",
    "        if e.text in kw_freq:\n",
    "            kw_freq[e.text] += 1\n",
    "        else:\n",
    "            kw_freq[e.text] = 1\n",
    "\n",
    "print(kw_freq, copy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature Set Services': 1, 'Red Color': 1, 'Msp': 1, 'value': 7, '   ': 1, 'service': 1, 'protection': 1, 'support': 3, 'development': 1, 'total': 3, 'security': 3, 'simple': 2, 'useful': 1, 'performance': 1, 'innovation': 1, 'intigration': 1, 'bundle': 1, 'totalsecurity': 1, 'integration': 2, 'features': 1}\n"
     ]
    }
   ],
   "source": [
    "# get lemma keywords \n",
    "# join the rest of the words together: \n",
    "from hunspell import Hunspell\n",
    "h = Hunspell();\n",
    "\n",
    "corpus = nlp(' '.join(copy_docs))\n",
    "\n",
    "MIN_CHARACTERS = 3\n",
    "\n",
    "for token in corpus:\n",
    "    if len(token.lemma_) >= MIN_CHARACTERS:\n",
    "        word = token.lemma_\n",
    "        if word.lower == word:\n",
    "            if not h.spell(token.lemma_):\n",
    "                if len(h.suggest(token.lemma_)) > 0:\n",
    "                    word = h.suggest(token.lemma_)[0]\n",
    "        if word in kw_freq:\n",
    "            kw_freq[word] += 1\n",
    "        else:\n",
    "            kw_freq[word] = 1\n",
    "\n",
    "print(kw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature Set Services': 1, 'Red Color': 1, 'Msp': 1, 'value': 7, '   ': 1, 'service': 1, 'protection': 1, 'support': 3, 'development': 1, 'total': 3, 'security': 3, 'simple': 2, 'useful': 1, 'performance': 1, 'innovation': 1, 'intigration': 1, 'bundle': 1, 'totalsecurity': 1, 'integration': 2, 'features': 1}\n"
     ]
    }
   ],
   "source": [
    "# proper casing\n",
    "\n",
    "caseless_freq = {}\n",
    "propercase_freq = {}\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if kw in caseless_freq:\n",
    "        caseless_freq[kw.lower()].append(count)\n",
    "    else:\n",
    "        caseless_freq[kw.lower()] = [count]\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if count == max(caseless_freq[kw.lower()]):\n",
    "        propercase_freq[kw] = sum(caseless_freq[kw.lower()])\n",
    "\n",
    "print(propercase_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 300) ['Feature Set Services', 'Red Color', 'Msp', 'value', '   ', 'service', 'protection', 'support', 'development', 'total', 'security', 'simple', 'useful', 'performance', 'innovation', 'intigration', 'bundle', 'totalsecurity', 'integration', 'features']\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = []\n",
    "labels_array = []\n",
    "\n",
    "for kw, count in propercase_freq.items():\n",
    "    labels_array.append(kw)\n",
    "    if nlp(kw)[0].vector.any() :\n",
    "        glove_vectors.append(nlp(kw)[0].vector)\n",
    "    else:\n",
    "        glove_vectors.append(np.array([0]*300))\n",
    "print(np.array(glove_vectors).shape, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Feature Set Services': 4, 'Red Color': 1, 'Msp': 0, 'value': 1, '   ': 1, 'service': 1, 'protection': 2, 'support': 1, 'development': 3, 'total': 1, 'security': 2, 'simple': 1, 'useful': 1, 'performance': 1, 'innovation': 3, 'intigration': 1, 'bundle': 1, 'totalsecurity': 1, 'integration': 3, 'features': 4}\n"
     ]
    }
   ],
   "source": [
    "# AffinityPropagation clustering \n",
    "\n",
    "AffinityPropagation_model = AffinityPropagation()\n",
    "AffinityPropagation_model.fit(glove_vectors)\n",
    "\n",
    "cluster_labels    = AffinityPropagation_model.labels_\n",
    "\n",
    "clusters = {}\n",
    "kw_cluster = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "    kw_cluster[labels_array[i]] = cluster_labels[i]\n",
    "\n",
    "print (kw_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# k means clustering \\n\\nkmeans_model = KMeans(init='k-means++', n_clusters=4, n_init=5)\\nkmeans_model.fit(glove_vectors)\\n\\n\\ncluster_labels    = kmeans_model.labels_\\n\\nclusters = {}\\nkw_cluster = {}\\nfor i in range(len(labels_array)):\\n    if cluster_labels[i] not in clusters:\\n        clusters[cluster_labels[i]] = [labels_array[i]]\\n    else:\\n        clusters[cluster_labels[i]].append(labels_array[i])\\n    kw_cluster[labels_array[i]] = cluster_labels[i]\\n\\nprint (kw_cluster)\\n\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# k means clustering \n",
    "\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=4, n_init=5)\n",
    "kmeans_model.fit(glove_vectors)\n",
    "\n",
    "\n",
    "cluster_labels    = kmeans_model.labels_\n",
    "\n",
    "clusters = {}\n",
    "kw_cluster = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "    kw_cluster[labels_array[i]] = cluster_labels[i]\n",
    "\n",
    "print (kw_cluster)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in true_divide\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n"
     ]
    }
   ],
   "source": [
    "#distance matrix (len(cluster_labels)^2)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "n = len(labels_array)\n",
    "\n",
    "distance_matrix = np.zeros([n, n])\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        distance_matrix[i][j] = spatial.distance.cosine(glove_vectors[i], glove_vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': 80, 'support': 46, 'total': 46, 'security': 46, 'simple': 38, 'integration': 38, 'Feature Set Services': 30, 'Red Color': 30, 'Msp': 30, '   ': 30, 'service': 30, 'protection': 30, 'development': 30, 'useful': 30, 'performance': 30, 'innovation': 30, 'intigration': 30, 'bundle': 30, 'totalsecurity': 30, 'features': 30}\n"
     ]
    }
   ],
   "source": [
    "# assign max font size\n",
    "\n",
    "def assign_font_size(propercase_freq, max_size, min_size):\n",
    "    label_fs = {}\n",
    "    sorted_tuples = [(k, propercase_freq[k]) for k in sorted(propercase_freq, key=propercase_freq.get, reverse=True)]\n",
    "    min_count = sorted_tuples[-1][1]\n",
    "    max_count = sorted_tuples[0][1]\n",
    "    \n",
    "    for kw, count in sorted_tuples:\n",
    "        if (max_count - min_count) == 0:\n",
    "            size = int((max_size - min_size) / 2.0 + min_size)\n",
    "        else:\n",
    "            #size = int(min_size + (max_size - min_size) * (count * 1.0 / (max_count - min_count)) ** 0.8)\n",
    "            size = int((max_size - min_size)/(max_count - min_count)*count + min_size - (max_size - min_size)/(max_count - min_count)*min_count)\n",
    "        label_fs[kw] = size\n",
    "    \n",
    "    return (label_fs)\n",
    "        \n",
    "kw_fs = assign_font_size(propercase_freq, 80, 30) #keyword_font_size\n",
    "print(kw_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'value': (260, 80), 'support': (209, 46), 'total': (149, 46), 'security': (239, 46), 'simple': (148, 38), 'integration': (271, 38), 'Feature Set Services': (390, 30), 'Red Color': (175, 30), 'Msp': (58, 30), '   ': (58, 30), 'service': (136, 30), 'protection': (195, 30), 'development': (214, 30), 'useful': (117, 30), 'performance': (214, 30), 'innovation': (195, 30), 'intigration': (214, 30), 'bundle': (117, 30), 'totalsecurity': (253, 30), 'features': (156, 30)}\n"
     ]
    }
   ],
   "source": [
    "def max_dimensions(kw_fs):\n",
    "    kw_dimensions = {}\n",
    "    for kw, fs in kw_fs.items():\n",
    "        kw_dimensions[kw] = (int(0.65*len(kw)*fs), fs) #x, y (i.e. width, height)\n",
    "    return kw_dimensions\n",
    "\n",
    "kw_max_dim = max_dimensions(kw_fs)\n",
    "print(kw_max_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, size, font_size, cluster):\n",
    "        self.word = word\n",
    "        self.width = size[\"width\"] #{width, height}\n",
    "        self.height = size[\"height\"]\n",
    "        self.font_size = font_size\n",
    "        self.cluster = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Cloud:\n",
    "    def __init__(self, words=[], canvas_size={\"x\": 1920, \"y\": 1080}, filename='clouds.html'):\n",
    "        self.words = words\n",
    "        self.canvas = [] #{word, font_size, x, y, width, height, color, cluster} <== color to be added\n",
    "        self.canvas_size = canvas_size\n",
    "        self.clusters = self.generate_clusters() # {0 : cluster0, 1 : cluster1, ...etc}\n",
    "        self.filename = filename\n",
    "        self.colors = [\"#6F694E\", \"#65D0B2\", \"#D8F546\", \"#FF724B\", \"#D6523E\", \"#B3F0E6\", \"#EAF380\", \"#A7328E\", \"#33DB45\", \"#EAEA45\", \"#63FFF3\", \"#7488AC\", \"#C0F8E1\"]\n",
    "        self.positions = []\n",
    "        \n",
    "    def generate_clusters(self):\n",
    "        clusters = {}\n",
    "        for w in self.words:\n",
    "            if w.cluster in clusters:\n",
    "                clusters[w.cluster].append(w)\n",
    "            else:\n",
    "                clusters[w.cluster] = [w]\n",
    "        return clusters\n",
    "    \n",
    "    '''\n",
    "    def choose_cluster_start(self):\n",
    "        start_points = {}\n",
    "        start_point = {}\n",
    "        r = 0\n",
    "        for i in range(len(self.clusters)):\n",
    "            c = self.clusters[i]\n",
    "            n = len(c)\n",
    "            \n",
    "            H = self.canvas_size[\"y\"] #total height\n",
    "            L = self.canvas_size[\"x\"] #total length\n",
    "            \n",
    "            if i%2 == 0:\n",
    "                y = random.randint(int(0.1*H), int(0.55*H))\n",
    "            else:\n",
    "                y = random.randint(int(0.55*H), int(0.9*H))\n",
    "            x = random.randint(int(r*L), min(int((r+len(c)/len(self.words))*L), int(L*0.90)))\n",
    "            \n",
    "            r = min(0.85, r + len(c)/len(self.words))\n",
    "            start_points[c[0].cluster] = {\n",
    "                \"x\": x,\n",
    "                \"y\": y\n",
    "            }\n",
    "        return start_points\n",
    "    '''\n",
    "        \n",
    "    def create_cloud(self):\n",
    "        \n",
    "        # sort by cluster size\n",
    "        # sort by max font-size\n",
    "        cl_size = {}\n",
    "        for c, words in self.clusters.items():\n",
    "            if len(words) < 4:\n",
    "                avg_size = sum([w.font_size for w in words])//len(words)\n",
    "            else:\n",
    "                avg_size = sum(sorted([w.font_size for w in words])[::-1][:4])/4\n",
    "            cl_size[c] = avg_size*3 - len(words)\n",
    "        sorted_clusters = sorted(cl_size, key=cl_size.get)[::-1]\n",
    "        \n",
    "        start_position = { \"x\": self.canvas_size[\"x\"]//2, \"y\": self.canvas_size[\"y\"]//2 }\n",
    "        \n",
    "        for i in range(len(sorted_clusters)):\n",
    "            c = sorted_clusters[i]\n",
    "            words = self.clusters[c]\n",
    "            self.positions = self.spiral(start_position)\n",
    "            \n",
    "            for w in words:\n",
    "                new_position = self.add_word_to_cloud(w) \n",
    "            \n",
    "            max_left_cloud = min([c[\"x\"] for c in self.canvas])\n",
    "            max_right_cloud = max([c[\"x\"] for c in self.canvas])\n",
    "            shift = 30\n",
    "            if i%2 == 0:\n",
    "                if new_position[\"x\"] < self.canvas_size[\"x\"]//2: \n",
    "                    start_position = { \"x\" : min(self.canvas_size[\"x\"]//2 + new_position[\"x\"], max_right_cloud + shift), \"y\": new_position[\"y\"] }\n",
    "                if new_position[\"x\"] > self.canvas_size[\"x\"]//2: \n",
    "                    start_position = { \"x\" : max(self.canvas_size[\"x\"] - new_position[\"x\"], max_left_cloud - shift), \"y\": new_position[\"y\"] }\n",
    "            #else:\n",
    "            start_position = new_position\n",
    "        \n",
    "        self.center_cloud()\n",
    "        \n",
    "    def draw_cloud_to_svg(self):\n",
    "        f = open(self.filename, 'w')\n",
    "        f.write('<svg viewbox=\"0 0 {} {}\" style=\"background: black\">'.format(self.canvas_size[\"x\"], self.canvas_size[\"y\"]))\n",
    "        for w in self.canvas:\n",
    "           \n",
    "\n",
    "            #f.write(' <rect x=\"{}\" y=\"{}\" width=\"{}\" height=\"{}\"/>'.format( w[\"x\"], w[\"y\"], w[\"width\"], w[\"height\"]))\n",
    "            f.write('<text x=\"{}\" y=\"{}\" font-family=\"Verdana\" font-size=\"{}\" fill=\"{}\">'.format(w[\"x\"], w[\"y\"], w[\"font_size\"], w[\"color\"]))\n",
    "            f.write(w[\"word\"])\n",
    "            f.write('</text>\\n')\n",
    "        f.write('</svg>')\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    def add_word_to_cloud(self, word): # word class Word\n",
    "        center = {\"x\": self.canvas_size[\"x\"] // 2, \"y\": self.canvas_size[\"y\"] // 2}\n",
    "        for p in self.positions:\n",
    "            if p[\"x\"] < center[\"x\"]:\n",
    "                if not self.verify_overlap( word, {\"x\": p[\"x\"] - word.width, \"y\": p[\"y\"]} ):\n",
    "                    self.canvas.append({\n",
    "                        \"word\": word.word,\n",
    "                        \"x\": p[\"x\"] - word.width,\n",
    "                        \"y\": p[\"y\"],\n",
    "                        \"width\": word.width,\n",
    "                        \"height\": word.height,\n",
    "                        \"font_size\": word.font_size,\n",
    "                        \"color\": self.colors[word.cluster],\n",
    "                        \"cluster\": word.cluster\n",
    "                    })\n",
    "                    self.positions.remove(p)\n",
    "                    return p\n",
    "            else:\n",
    "                if not self.verify_overlap( word, {\"x\": p[\"x\"], \"y\": p[\"y\"]} ):\n",
    "                    self.canvas.append({\n",
    "                        \"word\": word.word,\n",
    "                        \"x\": p[\"x\"],\n",
    "                        \"y\": p[\"y\"],\n",
    "                        \"width\": word.width,\n",
    "                        \"height\": word.height,\n",
    "                        \"font_size\": word.font_size,\n",
    "                        \"color\": self.colors[word.cluster],\n",
    "                        \"cluster\": word.cluster\n",
    "                    })\n",
    "                    self.positions.remove(p)\n",
    "                    return p\n",
    "\n",
    "        return self.positions[-1]\n",
    "            \n",
    "\n",
    "    def rect_intersection(self, r1, r2):\n",
    "        p1 = {}\n",
    "        p1[\"x\"] = r1[\"x\"]\n",
    "        p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "\n",
    "        p2 = {}\n",
    "        p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "        p2[\"y\"] = r1[\"y\"]\n",
    "\n",
    "        p3 = {}\n",
    "        p3[\"x\"] = r2[\"x\"]\n",
    "        p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "\n",
    "        p4 = {}\n",
    "        p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "        p4[\"y\"] = r2[\"y\"]\n",
    "\n",
    "        return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "    \n",
    "    def verify_overlap(self, word, position): # true if overlaps, false if not\n",
    "        new_rect = {\n",
    "            \"x\": position[\"x\"],\n",
    "            \"y\": position[\"y\"],\n",
    "            \"width\": word.width,\n",
    "            \"height\": word.height\n",
    "        }\n",
    "        for filled_rect in self.canvas:\n",
    "            if self.rect_intersection(filled_rect, new_rect):\n",
    "                return True\n",
    "        #verify out of bound of rectangle:\n",
    "        if new_rect[\"x\"] < 0 or new_rect[\"x\"] + new_rect[\"width\"] > self.canvas_size[\"x\"] or new_rect[\"y\"] > 1080 or new_rect[\"y\"]- new_rect[\"height\"] < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def spiral(self, start_point): # returns an [] with positions to test \n",
    "        points = [start_point]\n",
    "        # x = (a + b*theta)cos(theta)\n",
    "        # y = (a + b*theta)sin(theta)\n",
    "\n",
    "        # b = a final - a ini / 2 pi n  n=number of turns\n",
    "        a_ini = 0\n",
    "        # a_final = self.canvas_size[\"x\"]*len(self.clusters[cluster])/len(self.words) #spiral radius \n",
    "        a_final = self.canvas_size[\"x\"] #spiral radius \n",
    "\n",
    "        b = (a_final - a_ini)/(2*3.14159*(self.canvas_size[\"x\"]/70))\n",
    "\n",
    "        thetas = [ (self.canvas_size[\"y\"]/10 * 2)/1000 *x for x in range(1000)]\n",
    "        for i in thetas: #1000 points\n",
    "            x = ( a_ini + b*i + cos(i)*b/10)*cos(i) + start_point[\"x\"]\n",
    "            y = ( a_ini + b*i + cos(i)*b/10)*sin(i) + start_point[\"y\"]\n",
    "            points.append({\"x\": x, \"y\": y})\n",
    "\n",
    "        return points\n",
    "    \n",
    "    def center_cloud(self):\n",
    "        xs = [c[\"x\"] for c in self.canvas]\n",
    "        ys = [c[\"y\"] for c in self.canvas]\n",
    "        \n",
    "        x_min = min(xs)\n",
    "        x_max = max(xs) # ! not real max, real max need word width\n",
    "        \n",
    "        y_min = min(ys)\n",
    "        y_max = max(ys)\n",
    "        \n",
    "        shift_x = x_min - (self.canvas_size[\"x\"] - (x_max - x_min))//2\n",
    "        shift_y = y_min - (self.canvas_size[\"y\"] - (y_max - y_min))//2\n",
    "        \n",
    "        for c in self.canvas:\n",
    "            c[\"x\"] -= shift_x\n",
    "            c[\"y\"] -= shift_y\n",
    "        \n",
    "        \n",
    "    '''\n",
    "    def compress(self):\n",
    "        # pull words towards the one zith the most occurence\n",
    "        # create line \n",
    "        # test positions along that line \n",
    "        sizes = []\n",
    "        for w in self.canvas:\n",
    "            sizes.append(w[\"font_size\"])\n",
    "        central_word = self.canvas[sizes.index(max(sizes))]\n",
    "        \n",
    "        for w in self.canvas:\n",
    "            if w[\"cluster\"] != central_word[\"cluster\"]:\n",
    "                # sort tham by distance \n",
    "                pos_central_word = np.array([central_word[\"x\"], central_word[\"y\"]])\n",
    "                pos_w = np.array([w[\"x\"], w[\"y\"]])\n",
    "                dist = numpy.sqrt(numpy.sum((pos_central_word - pos_w)**2))\n",
    "                \n",
    "                # draw line \n",
    "                # inch closer \n",
    "                coeff = central_word[\"y\"] - w[\"y\"] / central_word[\"x\"] - w[\"x\"]\n",
    "                coordiantes = [{\"x\": central_word[\"x\"] + (central_word[\"x\"] - w[\"x\"])/100 * i, \"y\": central_word[\"y\"] + coeff* (central_word[\"x\"] - w[\"x\"])/100 * i } for i in range(100)]\n",
    "                for c in coordinates:\n",
    "                    for word in self.words:\n",
    "                        if self.verify_overlap(word, c):\n",
    "                            break\n",
    "    '''             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10,10))\\n\\nax = fig.add_subplot(111)\\nfig.subplots_adjust(top=0.85)\\n\\nfor w in cloud.canvas:\\n    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\\n\\nax.axis([0, 1920, 0, 1080])\\n\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for kw, d in kw_max_dim.items():\n",
    "    words.append(Word(kw, {\"width\": d[0], \"height\": d[1]}, kw_fs[kw], kw_cluster[kw]))\n",
    "\n",
    "cloud = Cloud(words=words)\n",
    "\n",
    "cloud.create_cloud()\n",
    "\n",
    "cloud.draw_cloud_to_svg()\n",
    "#cloud.compress()\n",
    "\n",
    "#print(cloud.canvas)\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "for w in cloud.canvas:\n",
    "    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\n",
    "\n",
    "ax.axis([0, 1920, 0, 1080])\n",
    "\n",
    "'''\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    def __init__(self, node):\n",
    "        self.id = node # we have a dict {id : { word, weight } }\n",
    "        self.adjacent = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])\n",
    "\n",
    "    def add_neighbor(self, neighbor, weight=0):\n",
    "        self.adjacent[neighbor] = weight\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.adjacent.keys()  \n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, neighbor):\n",
    "        return self.adjacent[neighbor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.vert_dict = {}\n",
    "        self.num_vertices = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_dict.values())\n",
    "\n",
    "    def add_vertex(self, node):\n",
    "        self.num_vertices = self.num_vertices + 1\n",
    "        new_vertex = Vertex(node)\n",
    "        self.vert_dict[node] = new_vertex\n",
    "        return new_vertex\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_dict:\n",
    "            return self.vert_dict[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_edge(self, frm, to, cost = 0):\n",
    "        if frm not in self.vert_dict:\n",
    "            self.add_vertex(frm)\n",
    "        if to not in self.vert_dict:\n",
    "            self.add_vertex(to)\n",
    "\n",
    "        self.vert_dict[frm].add_neighbor(self.vert_dict[to], cost)\n",
    "        self.vert_dict[to].add_neighbor(self.vert_dict[frm], cost)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_dict.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Star Forest Clustering and putting together\n",
    "'''\n",
    "\n",
    "def StarForestAlgo(g):\n",
    "    '''\n",
    "    g = similarity graph\n",
    "    '''\n",
    "    stars = []\n",
    "    while True:\n",
    "        usedVertices = []\n",
    "        bestCenter = findBestCenter(g, usedVertices)\n",
    "\n",
    "        if bestCenter is None:\n",
    "            break; \n",
    "        \n",
    "        star, usedVertices = createGraphStar(g, bestCenter, usedVertices) # graph, vertice, [vertices]\n",
    "        print(usedVertices)\n",
    "        stars.append(star)\n",
    "        \n",
    "        \n",
    "    return stars\n",
    "\n",
    "\n",
    "def findBestCenter(g, usedVertices): # graph, [vertices]\n",
    "    best_sum = 0\n",
    "    best_center = None\n",
    "    for v in g.get_vertices():\n",
    "        if v not in usedVertices:\n",
    "            sum = getSumOfConnectedEdges(g, v, usedVertices)\n",
    "            if sum > best_sum:\n",
    "                best_center = v\n",
    "    return best_center\n",
    "\n",
    "\n",
    "def getSumOfConnectedEdges(g, v, usedVertices):\n",
    "    sum = 0\n",
    "    connections = g.get_vertex(v).get_connections()\n",
    "    for c in connections:\n",
    "        if c not in usedVertices:\n",
    "            sum += g.get_vertex(v).get_weight(c)\n",
    "    return sum\n",
    "    \n",
    "\n",
    "def createGraphStar(g, bestCenter, usedVertices):\n",
    "    star = Graph()\n",
    "    for v in g.get_vertex(bestCenter).get_connections():\n",
    "        if v not in usedVertices and g.get_vertex(bestCenter) != v:\n",
    "            star.add_edge(bestCenter, v, g.get_vertex(bestCenter).get_weight(v))\n",
    "            print(v)\n",
    "            usedVertices.append(v)\n",
    "    return star, usedVertices\n",
    "\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.add_vertex('a')\n",
    "g.add_vertex('b')\n",
    "g.add_vertex('c')\n",
    "g.add_vertex('d')\n",
    "g.add_vertex('e')\n",
    "g.add_vertex('f')\n",
    "\n",
    "g.add_edge('a', 'b', 7)  \n",
    "g.add_edge('a', 'c', 9)\n",
    "g.add_edge('a', 'f', 14)\n",
    "g.add_edge('b', 'c', 10)\n",
    "g.add_edge('b', 'd', 15)\n",
    "g.add_edge('c', 'd', 11)\n",
    "g.add_edge('c', 'f', 2)\n",
    "g.add_edge('d', 'e', 6)\n",
    "g.add_edge('e', 'f', 9)\n",
    "\n",
    "#StarForestAlgo(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aspect ration of words : font_size (length) font_size*0.7(width)\n",
    "# Aspect ration of SVG file is 16:9\n",
    "\n",
    "# How to draw V1 \n",
    "# Create a polygon with the number of vertices = number of clusters \n",
    "# here cluster size = 3 so a triangle (not ever going to exceed 5)\n",
    "# 3 rectangles to fit within the first rectangle \n",
    "\n",
    "# in a 16:9\n",
    "\n",
    "# Cluster one in rect 1 (y = 16, 9/4) (w: 8, l: 9/2)\n",
    "# cluster Two in rect 2 (y = 16, 9/4*3) (w: 8, l: 9/2)\n",
    "# Cluster three in rect 3 (y = 8, 9/4) Biggest cluster ? (w: 8, l: 9/2)\n",
    "\n",
    "# Where to put the words \n",
    "# Start with the highest frequence with the biggest font : assign max font size before starting to draw\n",
    "# If the next one is smaller in frequence, font size drops by \n",
    "# font size 35 to 18\n",
    "# random choice where the word fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    def seam_carving(self):\n",
    "        board = self.make_board()\n",
    "        sparse = 0\n",
    "        while sparse < 500:\n",
    "            print(sparse)\n",
    "            sparse += 1\n",
    "            self.find_and_remove_path(board)\n",
    "            board = self.make_board()\n",
    "            \n",
    "    def find_and_remove_path(self, board):\n",
    "        v_path = []\n",
    "        h_path = []\n",
    "        sparse = True\n",
    "        for y in range(self.canvas_size[\"y\"]):\n",
    "            pt = {}\n",
    "            pt[\"y\"] = y\n",
    "            pt[\"x\"] = np.argmin(board[y])\n",
    "            v_path.append(pt) # first step \n",
    "            \n",
    "        for x in range(self.canvas_size[\"x\"]):\n",
    "            pt = {}\n",
    "            pt[\"x\"] = x\n",
    "            pt[\"y\"] = np.argmin(board[:,x])\n",
    "            h_path.append(pt) # first step \n",
    "        \n",
    "        for p in v_path + h_path:\n",
    "            for w in self.canvas:\n",
    "                if w[\"x\"] > p[\"x\"]:\n",
    "                    w[\"x\"]-= 1\n",
    "                if w[\"y\"] > p[\"y\"]:\n",
    "                    w[\"y\"] -= 1\n",
    "                    \n",
    "        board = self.make_board()\n",
    "        return sparse\n",
    "\n",
    "    \n",
    "    def make_board(self):\n",
    "        cv = self.canvas\n",
    "        # map canvas to a 1920 1080 matrix \n",
    "        board = np.zeros(shape=(1080, 1920)) #lines, columns\n",
    "        for w in cv:\n",
    "            for i in range(int(w[\"y\"]) - int(w[\"height\"]), int(w[\"y\"])+1):\n",
    "                board[i][int(w[\"x\"]) : int(w[\"x\"]) + int(w[\"width\"]) +1 ] = 1\n",
    "        return board\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
