{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from math import cos\n",
    "from math import sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers to holidays\n",
    "docs = [\n",
    "    'I went to France to visit Paris',\n",
    "    'New York City was crowded',\n",
    "    'sightseeing in Bali',\n",
    "    'swimming in the tropical oceans',\n",
    "    'swimming in Bali',\n",
    "    'raining in Paris',\n",
    "    'visiting monuments in Paris',\n",
    "    'San Francisco',\n",
    "    'San Francisco',\n",
    "    'flew to Berlin',\n",
    "    'enjoyed visiting eiffel tower in paris',\n",
    "    'Paris, city of love',\n",
    "    'Flying is fun',\n",
    "    'Paris is crowded',\n",
    "    'France'\n",
    "]\n",
    "\n",
    "docs = [x.strip('\\n') for x in open('answers.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped docs [become vegetarian, use electric cars, use public transport, Make big companies pollute less, use solar, clean energy less meat less burger shorter shower bath dry toilet electric car, reduce carbon emissions, overconsumption planned obsolescence, tes qwd asd asd asdasd, Pollute less Renewable energy Better industies, ecology renewable energy, Cleean energies, Clean energies, bike work, pollute less, use reusable items, Power home renewable energy, Invest energy efficient appliances, Reduce water waste, eat food buy make less meat, fuel efficient vehicle, nuclear fusion wind turbines solar pannels tidal turbines education, green ecology electric solar wind nuclear chemtrails, Shrink carbon profile, Sensibilisation research, Replace regular incandescent light bulb, Drive less Carpool, Reduce Reuse Recycle, Go Solar, buy energy efficient appliances, Reduce waste, Avoid products packaging, Turn lights, Turn Electronic Devices, Plant Tree, Use Clean Fuel, education, education, teach, electric cars, Renewable Fuel, Go Green, use renewable energies, eat less hamburger, nuclear fusion, renewable energies, ride bike, reuse towels, take lunch Tupperware, reduce fossil fuel use, Plant trees, Reduce Waste, Conserve water, Reduce Reuse Recycle, Use less heat Air Conditioning, Drive less Drive smart, buy energy efficient products, Plant Tree, use clean energy]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "def stop_word_stripper(line):\n",
    "    stop_words = [w.strip('\\n').lower() for w in open('stop_words.txt').readlines()]\n",
    "    pos_stopper = ['PUNCT', 'SYM']\n",
    "    return ' '.join([token.text for token in line if str(token).lower() not in stop_words and token.pos_  not in pos_stopper])\n",
    "\n",
    "stripped_docs = [] #spacy object\n",
    "copy_docs = [] # strings\n",
    "for d in docs:\n",
    "    stripped_docs.append(nlp(stop_word_stripper(nlp(d))))\n",
    "    copy_docs.append(stop_word_stripper(nlp(d)))\n",
    "    \n",
    "print('stripped docs', stripped_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1} ['become vegetarian', 'use electric cars', 'use public transport', 'Make big companies pollute less', 'use solar', 'clean energy less meat less burger shorter shower bath dry toilet electric car', 'reduce carbon emissions', 'overconsumption planned obsolescence', 'tes qwd asd asd asdasd', 'Pollute less Renewable energy Better industies', 'ecology renewable energy', 'Cleean energies', 'Clean energies', 'bike work', 'pollute less', 'use reusable items', 'Power home renewable energy', 'Invest energy efficient appliances', 'Reduce water waste', 'eat food buy make less meat', 'fuel efficient vehicle', 'nuclear fusion wind turbines solar pannels tidal turbines education', 'green ecology electric solar wind nuclear chemtrails', 'Shrink carbon profile', 'Sensibilisation research', 'Replace regular incandescent light bulb', 'Drive less Carpool', 'Reduce Reuse Recycle', 'Go Solar', 'buy energy efficient appliances', 'Reduce waste', 'Avoid products packaging', 'Turn lights', 'Turn Electronic Devices', 'Plant Tree', 'Use Clean Fuel', 'education', 'education', 'teach', 'electric cars', 'Renewable Fuel', 'Go Green', 'use renewable energies', 'eat less hamburger', 'nuclear fusion', 'renewable energies', 'ride bike', 'reuse towels', 'take lunch', 'reduce fossil fuel use', 'Plant trees', 'Reduce Waste', 'Conserve water', 'Reduce Reuse Recycle', 'Use less heat', 'Drive less Drive smart', 'buy energy efficient products', 'Plant Tree', 'use clean energy']\n"
     ]
    }
   ],
   "source": [
    "# parse through to get entities \n",
    "kw_freq = {}\n",
    "\n",
    "for i in range(len(stripped_docs)):\n",
    "    line = stripped_docs[i]\n",
    "    for e in line.ents:\n",
    "        copy_docs[i] = copy_docs[i].replace(e.text, '').strip()\n",
    "        if e.text in kw_freq:\n",
    "            kw_freq[e.text] += 1\n",
    "        else:\n",
    "            kw_freq[e.text] = 1\n",
    "\n",
    "print(kw_freq, copy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1, 'become': 1, 'vegetarian': 1, 'electric': 4, 'public': 1, 'transport': 1, 'make': 2, 'company': 1, 'pollute': 3, 'less': 10, 'solar': 4, 'clean': 4, 'energy': 12, 'meat': 2, 'burger': 1, 'short': 1, 'shower': 1, 'bath': 1, 'toilet': 1, 'reduce': 7, 'carbon': 2, 'emission': 1, 'overconsumption': 1, 'plan': 1, 'obsolescence': 1, 'asdasd': 1, 'renewable': 6, 'better': 1, 'industie': 1, 'ecology': 2, 'cleean': 1, 'bike': 2, 'work': 1, 'reusable': 1, 'item': 1, 'power': 1, 'home': 1, 'invest': 1, 'efficient': 4, 'appliance': 2, 'water': 2, 'waste': 3, 'food': 1, 'fuel': 4, 'vehicle': 1, 'nuclear': 3, 'fusion': 2, 'wind': 2, 'turbine': 2, 'pannel': 1, 'tidal': 1, 'education': 3, 'green': 2, 'chemtrail': 1, 'shrink': 1, 'profile': 1, 'sensibilisation': 1, 'research': 1, 'replace': 1, 'regular': 1, 'incandescent': 1, 'light': 2, 'bulb': 1, 'drive': 3, 'carpool': 1, 'reuse': 3, 'recycle': 2, 'avoid': 1, 'product': 2, 'packaging': 1, 'turn': 2, 'electronic': 1, 'devices': 1, 'plant': 3, 'tree': 3, 'teach': 1, 'hamburger': 1, 'ride': 1, 'towel': 1, 'take': 1, 'lunch': 1, 'fossil': 1, 'conserve': 1, 'heat': 1, 'smart': 1}\n"
     ]
    }
   ],
   "source": [
    "# get lemma keywords \n",
    "# join the rest of the words together: \n",
    "\n",
    "corpus = nlp(' '.join(copy_docs))\n",
    "\n",
    "MIN_CHARACTERS = 3\n",
    "\n",
    "for token in corpus:\n",
    "    if len(token.lemma_) > MIN_CHARACTERS:\n",
    "        if token.lemma_ in kw_freq:\n",
    "            kw_freq[token.lemma_] += 1\n",
    "        else:\n",
    "            kw_freq[token.lemma_] = 1\n",
    "\n",
    "print(kw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1, 'become': 1, 'vegetarian': 1, 'electric': 4, 'public': 1, 'transport': 1, 'make': 2, 'company': 1, 'pollute': 3, 'less': 10, 'solar': 4, 'clean': 4, 'energy': 12, 'meat': 2, 'burger': 1, 'short': 1, 'shower': 1, 'bath': 1, 'toilet': 1, 'reduce': 7, 'carbon': 2, 'emission': 1, 'overconsumption': 1, 'plan': 1, 'obsolescence': 1, 'asdasd': 1, 'renewable': 6, 'better': 1, 'industie': 1, 'ecology': 2, 'cleean': 1, 'bike': 2, 'work': 1, 'reusable': 1, 'item': 1, 'power': 1, 'home': 1, 'invest': 1, 'efficient': 4, 'appliance': 2, 'water': 2, 'waste': 3, 'food': 1, 'fuel': 4, 'vehicle': 1, 'nuclear': 3, 'fusion': 2, 'wind': 2, 'turbine': 2, 'pannel': 1, 'tidal': 1, 'education': 3, 'green': 2, 'chemtrail': 1, 'shrink': 1, 'profile': 1, 'sensibilisation': 1, 'research': 1, 'replace': 1, 'regular': 1, 'incandescent': 1, 'light': 2, 'bulb': 1, 'drive': 3, 'carpool': 1, 'reuse': 3, 'recycle': 2, 'avoid': 1, 'product': 2, 'packaging': 1, 'turn': 2, 'electronic': 1, 'devices': 1, 'plant': 3, 'tree': 3, 'teach': 1, 'hamburger': 1, 'ride': 1, 'towel': 1, 'take': 1, 'lunch': 1, 'fossil': 1, 'conserve': 1, 'heat': 1, 'smart': 1}\n"
     ]
    }
   ],
   "source": [
    "# proper casing\n",
    "\n",
    "caseless_freq = {}\n",
    "propercase_freq = {}\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if kw in caseless_freq:\n",
    "        caseless_freq[kw.lower()].append(count)\n",
    "    else:\n",
    "        caseless_freq[kw.lower()] = [count]\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if count == max(caseless_freq[kw.lower()]):\n",
    "        propercase_freq[kw] = sum(caseless_freq[kw.lower()])\n",
    "\n",
    "print(propercase_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 300) ['Tupperware', 'Air Conditioning', 'become', 'vegetarian', 'electric', 'public', 'transport', 'make', 'company', 'pollute', 'less', 'solar', 'clean', 'energy', 'meat', 'burger', 'short', 'shower', 'bath', 'toilet', 'reduce', 'carbon', 'emission', 'overconsumption', 'plan', 'obsolescence', 'asdasd', 'renewable', 'better', 'industie', 'ecology', 'cleean', 'bike', 'work', 'reusable', 'item', 'power', 'home', 'invest', 'efficient', 'appliance', 'water', 'waste', 'food', 'fuel', 'vehicle', 'nuclear', 'fusion', 'wind', 'turbine', 'pannel', 'tidal', 'education', 'green', 'chemtrail', 'shrink', 'profile', 'sensibilisation', 'research', 'replace', 'regular', 'incandescent', 'light', 'bulb', 'drive', 'carpool', 'reuse', 'recycle', 'avoid', 'product', 'packaging', 'turn', 'electronic', 'devices', 'plant', 'tree', 'teach', 'hamburger', 'ride', 'towel', 'take', 'lunch', 'fossil', 'conserve', 'heat', 'smart']\n"
     ]
    }
   ],
   "source": [
    "# semantic k means clustering\n",
    "\n",
    "glove_vectors = []\n",
    "labels_array = []\n",
    "\n",
    "for kw, count in propercase_freq.items():\n",
    "    labels_array.append(kw)\n",
    "    glove_vectors.append(nlp(kw)[0].vector)\n",
    "\n",
    "print(np.array(glove_vectors).shape, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=5, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k means clustering \n",
    "\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=4, n_init=5)\n",
    "kmeans_model.fit(glove_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 3, 'Air Conditioning': 0, 'become': 2, 'vegetarian': 1, 'electric': 0, 'public': 2, 'transport': 2, 'make': 2, 'company': 2, 'pollute': 2, 'less': 2, 'solar': 0, 'clean': 2, 'energy': 0, 'meat': 1, 'burger': 1, 'short': 2, 'shower': 2, 'bath': 2, 'toilet': 2, 'reduce': 2, 'carbon': 0, 'emission': 0, 'overconsumption': 2, 'plan': 2, 'obsolescence': 2, 'asdasd': 2, 'renewable': 0, 'better': 2, 'industie': 2, 'ecology': 0, 'cleean': 2, 'bike': 2, 'work': 2, 'reusable': 3, 'item': 2, 'power': 0, 'home': 2, 'invest': 2, 'efficient': 2, 'appliance': 2, 'water': 0, 'waste': 2, 'food': 1, 'fuel': 0, 'vehicle': 2, 'nuclear': 0, 'fusion': 2, 'wind': 0, 'turbine': 0, 'pannel': 2, 'tidal': 0, 'education': 2, 'green': 2, 'chemtrail': 2, 'shrink': 2, 'profile': 2, 'sensibilisation': 2, 'research': 2, 'replace': 2, 'regular': 2, 'incandescent': 0, 'light': 2, 'bulb': 2, 'drive': 2, 'carpool': 2, 'reuse': 3, 'recycle': 3, 'avoid': 2, 'product': 2, 'packaging': 3, 'turn': 2, 'electronic': 2, 'devices': 2, 'plant': 0, 'tree': 2, 'teach': 2, 'hamburger': 1, 'ride': 2, 'towel': 2, 'take': 2, 'lunch': 1, 'fossil': 0, 'conserve': 2, 'heat': 0, 'smart': 2}\n"
     ]
    }
   ],
   "source": [
    "cluster_labels    = kmeans_model.labels_\n",
    "cluster_inertia   = kmeans_model.inertia_\n",
    "\n",
    "clusters = {}\n",
    "kw_cluster = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "    kw_cluster[labels_array[i]] = cluster_labels[i]\n",
    "\n",
    "print (kw_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:643: RuntimeWarning: overflow encountered in square\n",
      "  vv = np.average(np.square(v), weights=w)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:642: RuntimeWarning: overflow encountered in square\n",
      "  uu = np.average(np.square(u), weights=w)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:641: RuntimeWarning: overflow encountered in multiply\n",
      "  uv = np.average(u * v, weights=w)\n"
     ]
    }
   ],
   "source": [
    "#distance matrix (len(cluster_labels)^2)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "n = len(labels_array)\n",
    "\n",
    "distance_matrix = np.zeros([n, n])\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        distance_matrix[i][j] = spatial.distance.cosine(glove_vectors[i], glove_vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': 65, 'less': 56, 'reduce': 44, 'renewable': 40, 'electric': 32, 'solar': 32, 'clean': 32, 'efficient': 32, 'fuel': 32, 'pollute': 28, 'waste': 28, 'nuclear': 28, 'education': 28, 'drive': 28, 'reuse': 28, 'plant': 28, 'tree': 28, 'make': 24, 'meat': 24, 'carbon': 24, 'ecology': 24, 'bike': 24, 'appliance': 24, 'water': 24, 'fusion': 24, 'wind': 24, 'turbine': 24, 'green': 24, 'light': 24, 'recycle': 24, 'product': 24, 'turn': 24, 'Tupperware': 20, 'Air Conditioning': 20, 'become': 20, 'vegetarian': 20, 'public': 20, 'transport': 20, 'company': 20, 'burger': 20, 'short': 20, 'shower': 20, 'bath': 20, 'toilet': 20, 'emission': 20, 'overconsumption': 20, 'plan': 20, 'obsolescence': 20, 'asdasd': 20, 'better': 20, 'industie': 20, 'cleean': 20, 'work': 20, 'reusable': 20, 'item': 20, 'power': 20, 'home': 20, 'invest': 20, 'food': 20, 'vehicle': 20, 'pannel': 20, 'tidal': 20, 'chemtrail': 20, 'shrink': 20, 'profile': 20, 'sensibilisation': 20, 'research': 20, 'replace': 20, 'regular': 20, 'incandescent': 20, 'bulb': 20, 'carpool': 20, 'avoid': 20, 'packaging': 20, 'electronic': 20, 'devices': 20, 'teach': 20, 'hamburger': 20, 'ride': 20, 'towel': 20, 'take': 20, 'lunch': 20, 'fossil': 20, 'conserve': 20, 'heat': 20, 'smart': 20}\n"
     ]
    }
   ],
   "source": [
    "# assign max font size\n",
    "\n",
    "def assign_font_size(propercase_freq, max_size, min_size):\n",
    "    label_fs = {}\n",
    "    sorted_tuples = [(k, propercase_freq[k]) for k in sorted(propercase_freq, key=propercase_freq.get, reverse=True)]\n",
    "    min_count = sorted_tuples[-1][1]\n",
    "    max_count = sorted_tuples[0][1]\n",
    "    \n",
    "    for kw, count in sorted_tuples:\n",
    "        if (max_count - min_count) == 0:\n",
    "            size = int((max_size - min_size) / 2.0 + min_size)\n",
    "        else:\n",
    "            #size = int(min_size + (max_size - min_size) * (count * 1.0 / (max_count - min_count)) ** 0.8)\n",
    "            size = int((max_size - min_size)/(max_count - min_count)*count + min_size - (max_size - min_size)/(max_count - min_count)*min_count)\n",
    "        label_fs[kw] = size\n",
    "    \n",
    "    return (label_fs)\n",
    "        \n",
    "kw_fs = assign_font_size(propercase_freq, 65, 20) #keyword_font_size\n",
    "print(kw_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': (272, 65), 'less': (156, 56), 'reduce': (184, 44), 'renewable': (252, 40), 'electric': (179, 32), 'solar': (112, 32), 'clean': (112, 32), 'efficient': (201, 32), 'fuel': (89, 32), 'pollute': (137, 28), 'waste': (98, 28), 'nuclear': (137, 28), 'education': (176, 28), 'drive': (98, 28), 'reuse': (98, 28), 'plant': (98, 28), 'tree': (78, 28), 'make': (67, 24), 'meat': (67, 24), 'carbon': (100, 24), 'ecology': (117, 24), 'bike': (67, 24), 'appliance': (151, 24), 'water': (84, 24), 'fusion': (100, 24), 'wind': (67, 24), 'turbine': (117, 24), 'green': (84, 24), 'light': (84, 24), 'recycle': (117, 24), 'product': (117, 24), 'turn': (67, 24), 'Tupperware': (140, 20), 'Air Conditioning': (224, 20), 'become': (83, 20), 'vegetarian': (140, 20), 'public': (83, 20), 'transport': (126, 20), 'company': (97, 20), 'burger': (83, 20), 'short': (70, 20), 'shower': (83, 20), 'bath': (56, 20), 'toilet': (83, 20), 'emission': (112, 20), 'overconsumption': (210, 20), 'plan': (56, 20), 'obsolescence': (167, 20), 'asdasd': (83, 20), 'better': (83, 20), 'industie': (112, 20), 'cleean': (83, 20), 'work': (56, 20), 'reusable': (112, 20), 'item': (56, 20), 'power': (70, 20), 'home': (56, 20), 'invest': (83, 20), 'food': (56, 20), 'vehicle': (97, 20), 'pannel': (83, 20), 'tidal': (70, 20), 'chemtrail': (126, 20), 'shrink': (83, 20), 'profile': (97, 20), 'sensibilisation': (210, 20), 'research': (112, 20), 'replace': (97, 20), 'regular': (97, 20), 'incandescent': (167, 20), 'bulb': (56, 20), 'carpool': (97, 20), 'avoid': (70, 20), 'packaging': (126, 20), 'electronic': (140, 20), 'devices': (97, 20), 'teach': (70, 20), 'hamburger': (126, 20), 'ride': (56, 20), 'towel': (70, 20), 'take': (56, 20), 'lunch': (70, 20), 'fossil': (83, 20), 'conserve': (112, 20), 'heat': (56, 20), 'smart': (70, 20)}\n"
     ]
    }
   ],
   "source": [
    "def max_dimensions(kw_fs):\n",
    "    kw_dimensions = {}\n",
    "    for kw, fs in kw_fs.items():\n",
    "        kw_dimensions[kw] = (int(0.7*len(kw)*fs), fs) #x, y (i.e. width, height)\n",
    "    return kw_dimensions\n",
    "\n",
    "kw_max_dim = max_dimensions(kw_fs)\n",
    "print(kw_max_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nr1 = {\\n    \"x\" : 0,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\n\\nr2 = {\\n    \"x\" : 5,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\nrect_intersection(r1, r2)\\n'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you need x,y width height for each rectangle (word)\n",
    "# r1 x, y, width, height \n",
    "# p1--------\n",
    "#  |        |\n",
    "#  |        |\n",
    "#  |--------p2\n",
    "\n",
    "def rect_intersection(r1, r2): #return true if intersects\n",
    "    p1 = {}\n",
    "    p1[\"x\"] = r1[\"x\"]\n",
    "    p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "    \n",
    "    p2 = {}\n",
    "    p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "    p2[\"y\"] = r1[\"y\"]\n",
    "    \n",
    "    p3 = {}\n",
    "    p3[\"x\"] = r2[\"x\"]\n",
    "    p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "    \n",
    "    p4 = {}\n",
    "    p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "    p4[\"y\"] = r2[\"y\"]\n",
    "    \n",
    "    return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "#test\n",
    "'''\n",
    "r1 = {\n",
    "    \"x\" : 0,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "\n",
    "r2 = {\n",
    "    \"x\" : 5,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "rect_intersection(r1, r2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, size, font_size, cluster):\n",
    "        self.word = word\n",
    "        self.width = size[\"width\"] #{width, height}\n",
    "        self.height = size[\"height\"]\n",
    "        self.font_size = font_size\n",
    "        self.cluster = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cloud:\n",
    "    def __init__(self, words=[], canvas_size={\"x\": 1920, \"y\": 1080}, filename='clouds.html'):\n",
    "        self.words = words\n",
    "        self.canvas = [] #{x, y, width, height, color} <== color to be added\n",
    "        self.canvas_size = canvas_size\n",
    "        #possible positions in relation to different clusters\n",
    "        self.clusters = self.generate_clusters() # {0 : cluster0, 1 : cluster1, ...etc}\n",
    "        self.filename = filename\n",
    "        self.possible_positions = self.generate_positions()\n",
    "        self.colors = [\"#5F0B2B\",\"#D11638\", \"#F08801\", \"#FACE00\", \"#ADA20B\", \"#D4D639\", \"#EF6D3E\", \"#90A6BF\", \"#F2AEB4\", \"#AA61AE\", \"#FBC6CE\"]\n",
    "        \n",
    "    def generate_clusters(self):\n",
    "        clusters = {}\n",
    "        for w in self.words:\n",
    "            if w.cluster in clusters:\n",
    "                clusters[w.cluster].append(w)\n",
    "            else:\n",
    "                clusters[w.cluster] = [w]\n",
    "        return clusters\n",
    "    \n",
    "    def choose_cluster_start(self):\n",
    "        start_points = {}\n",
    "        start_point = {}\n",
    "        r = 0\n",
    "        for i in range(len(self.clusters)):\n",
    "            c = self.clusters[i]\n",
    "            n = len(c)\n",
    "            \n",
    "            H = self.canvas_size[\"y\"] #total height\n",
    "            L = self.canvas_size[\"x\"] #total length\n",
    "            \n",
    "            if i%2 == 0:\n",
    "                y = random.randint(int(0.1*H), int(0.55*H))\n",
    "            else:\n",
    "                y = random.randint(int(0.55*H), int(0.9*H))\n",
    "            x = random.randint(int(r*L), min(int((r+len(c)/len(self.words))*L), int(L*0.90)))\n",
    "            \n",
    "            r = min(0.85, r + len(c)/len(self.words))\n",
    "            start_points[c[0].cluster] = {\n",
    "                \"x\": x,\n",
    "                \"y\": y\n",
    "            }\n",
    "        return start_points\n",
    "\n",
    "    \n",
    "    def generate_positions(self):\n",
    "        start_positions = self.choose_cluster_start() \n",
    "        positions = {}\n",
    "        for cluster, s_p in start_positions.items():\n",
    "            positions[cluster] = self.spiral(s_p, cluster)\n",
    "        \n",
    "        return positions\n",
    "    \n",
    "    \n",
    "    def create_cloud_svg(self):\n",
    "        for w in self.words:\n",
    "            self.add_word_to_cloud(w)\n",
    "        \n",
    "        f = open(self.filename, 'w')\n",
    "        f.write('<svg viewbox=\"0 0 1920 1080\">')\n",
    "        for w in cloud.canvas:\n",
    "            f.write('<text x=\"{}\" y=\"{}\" font-family=\"Verdana\" font-size=\"{}\" stroke=\"none\" fill=\"{}\">'.format(w[\"x\"], w[\"y\"], w[\"font_size\"], w[\"color\"]))\n",
    "            f.write(w[\"word\"])\n",
    "            f.write('</text>\\n')\n",
    "        f.write('</svg>')\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    def add_word_to_cloud(self, word): # word class Word\n",
    "        c = word.cluster\n",
    "        \n",
    "        for p in self.possible_positions[c]:\n",
    "            if self.verify_overlap( word, p):\n",
    "                self.possible_positions[c].remove(p)\n",
    "            else:\n",
    "                self.canvas.append({\n",
    "                    \"word\": word.word,\n",
    "                    \"x\": p[\"x\"],\n",
    "                    \"y\": p[\"y\"],\n",
    "                    \"width\": word.width,\n",
    "                    \"height\": word.height,\n",
    "                    \"font_size\": word.font_size,\n",
    "                    \"color\": self.colors[c] \n",
    "                })\n",
    "                self.possible_positions[c].remove(p)\n",
    "                break;\n",
    "\n",
    "\n",
    "    def rect_intersection(self, r1, r2):\n",
    "        p1 = {}\n",
    "        p1[\"x\"] = r1[\"x\"]\n",
    "        p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "\n",
    "        p2 = {}\n",
    "        p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "        p2[\"y\"] = r1[\"y\"]\n",
    "\n",
    "        p3 = {}\n",
    "        p3[\"x\"] = r2[\"x\"]\n",
    "        p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "\n",
    "        p4 = {}\n",
    "        p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "        p4[\"y\"] = r2[\"y\"]\n",
    "\n",
    "        return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "    \n",
    "    def verify_overlap(self, word, position): # true if overlaps, false if not\n",
    "        new_rect = {\n",
    "            \"x\": position[\"x\"],\n",
    "            \"y\": position[\"y\"],\n",
    "            \"width\": word.width,\n",
    "            \"height\": word.height\n",
    "        }\n",
    "        for filled_rect in self.canvas:\n",
    "            if self.rect_intersection(filled_rect, new_rect):\n",
    "                return True\n",
    "        #verify out of bound of rectangle:\n",
    "        if new_rect[\"x\"] < 0 or new_rect[\"x\"] + new_rect[\"width\"] > 1920 or new_rect[\"y\"] > 1080 or new_rect[\"y\"]- new_rect[\"height\"] < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def spiral(self, start_point, cluster): # returns an [] with positions to test \n",
    "        points = [start_point]\n",
    "        print(start_point)\n",
    "        # x = (a + b*theta)cos(theta)\n",
    "        # y = (a + b*theta)sin(theta)\n",
    "\n",
    "        # b = a final - a ini / 2 pi n  n=number of turns\n",
    "        a_ini = 0\n",
    "        a_final = self.canvas_size[\"x\"]*len(self.clusters[cluster])/len(self.words)\n",
    "\n",
    "        b = (a_final - a_ini)/(2*3.14159*(self.canvas_size[\"y\"]/10))\n",
    "\n",
    "        thetas = [ (self.canvas_size[\"y\"]/10 * 2)/1000 *x for x in range(1000)]\n",
    "        for i in thetas: #1000 points\n",
    "            x = ( a_ini + b*i )*cos(i) + start_point[\"x\"]\n",
    "            y = ( a_ini + b*i )*sin(i) + start_point[\"y\"]\n",
    "            points.append({\"x\": x, \"y\": y})\n",
    "\n",
    "        return points\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 4, 'y': 288}\n",
      "{'x': 512, 'y': 781}\n",
      "{'x': 1279, 'y': 403}\n",
      "{'x': 1648, 'y': 746}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10,10))\\n\\nax = fig.add_subplot(111)\\nfig.subplots_adjust(top=0.85)\\n\\nfor w in cloud.canvas:\\n    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\\n\\nax.axis([0, 1920, 0, 1080])\\n\\n'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for kw, d in kw_max_dim.items():\n",
    "    words.append(Word(kw, {\"width\": d[0], \"height\": d[1]}, kw_fs[kw], kw_cluster[kw]))\n",
    "\n",
    "cloud = Cloud(words=words)\n",
    "\n",
    "cloud.create_cloud_svg()\n",
    "    \n",
    "#print(cloud.canvas)\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "for w in cloud.canvas:\n",
    "    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\n",
    "\n",
    "ax.axis([0, 1920, 0, 1080])\n",
    "\n",
    "'''\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clouds.html', 'w')\n",
    "f.write('<svg viewbox=\"0 0 1920 1080\">')\n",
    "for w in cloud.canvas:\n",
    "    f.write('<text x=\"{}\" y=\"{}\" font-family=\"Verdana\" font-size=\"{}\">'.format(w[\"x\"], w[\"y\"], w[\"font_size\"]))\n",
    "    f.write(w[\"word\"])\n",
    "    f.write('</text>\\n')\n",
    "f.write('</svg>')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    def __init__(self, node):\n",
    "        self.id = node # we have a dict {id : { word, weight } }\n",
    "        self.adjacent = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])\n",
    "\n",
    "    def add_neighbor(self, neighbor, weight=0):\n",
    "        self.adjacent[neighbor] = weight\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.adjacent.keys()  \n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, neighbor):\n",
    "        return self.adjacent[neighbor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.vert_dict = {}\n",
    "        self.num_vertices = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_dict.values())\n",
    "\n",
    "    def add_vertex(self, node):\n",
    "        self.num_vertices = self.num_vertices + 1\n",
    "        new_vertex = Vertex(node)\n",
    "        self.vert_dict[node] = new_vertex\n",
    "        return new_vertex\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_dict:\n",
    "            return self.vert_dict[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_edge(self, frm, to, cost = 0):\n",
    "        if frm not in self.vert_dict:\n",
    "            self.add_vertex(frm)\n",
    "        if to not in self.vert_dict:\n",
    "            self.add_vertex(to)\n",
    "\n",
    "        self.vert_dict[frm].add_neighbor(self.vert_dict[to], cost)\n",
    "        self.vert_dict[to].add_neighbor(self.vert_dict[frm], cost)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_dict.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Star Forest Clustering and putting together\n",
    "'''\n",
    "\n",
    "def StarForestAlgo(g):\n",
    "    '''\n",
    "    g = similarity graph\n",
    "    '''\n",
    "    stars = []\n",
    "    while True:\n",
    "        usedVertices = []\n",
    "        bestCenter = findBestCenter(g, usedVertices)\n",
    "\n",
    "        if bestCenter is None:\n",
    "            break; \n",
    "        \n",
    "        star, usedVertices = createGraphStar(g, bestCenter, usedVertices) # graph, vertice, [vertices]\n",
    "        print(usedVertices)\n",
    "        stars.append(star)\n",
    "        \n",
    "        \n",
    "    return stars\n",
    "\n",
    "\n",
    "def findBestCenter(g, usedVertices): # graph, [vertices]\n",
    "    best_sum = 0\n",
    "    best_center = None\n",
    "    for v in g.get_vertices():\n",
    "        if v not in usedVertices:\n",
    "            sum = getSumOfConnectedEdges(g, v, usedVertices)\n",
    "            if sum > best_sum:\n",
    "                best_center = v\n",
    "    return best_center\n",
    "\n",
    "\n",
    "def getSumOfConnectedEdges(g, v, usedVertices):\n",
    "    sum = 0\n",
    "    connections = g.get_vertex(v).get_connections()\n",
    "    for c in connections:\n",
    "        if c not in usedVertices:\n",
    "            sum += g.get_vertex(v).get_weight(c)\n",
    "    return sum\n",
    "    \n",
    "\n",
    "def createGraphStar(g, bestCenter, usedVertices):\n",
    "    star = Graph()\n",
    "    for v in g.get_vertex(bestCenter).get_connections():\n",
    "        if v not in usedVertices and g.get_vertex(bestCenter) != v:\n",
    "            star.add_edge(bestCenter, v, g.get_vertex(bestCenter).get_weight(v))\n",
    "            print(v)\n",
    "            usedVertices.append(v)\n",
    "    return star, usedVertices\n",
    "\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.add_vertex('a')\n",
    "g.add_vertex('b')\n",
    "g.add_vertex('c')\n",
    "g.add_vertex('d')\n",
    "g.add_vertex('e')\n",
    "g.add_vertex('f')\n",
    "\n",
    "g.add_edge('a', 'b', 7)  \n",
    "g.add_edge('a', 'c', 9)\n",
    "g.add_edge('a', 'f', 14)\n",
    "g.add_edge('b', 'c', 10)\n",
    "g.add_edge('b', 'd', 15)\n",
    "g.add_edge('c', 'd', 11)\n",
    "g.add_edge('c', 'f', 2)\n",
    "g.add_edge('d', 'e', 6)\n",
    "g.add_edge('e', 'f', 9)\n",
    "\n",
    "#StarForestAlgo(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aspect ration of words : font_size (length) font_size*0.7(width)\n",
    "# Aspect ration of SVG file is 16:9\n",
    "\n",
    "# How to draw V1 \n",
    "# Create a polygon with the number of vertices = number of clusters \n",
    "# here cluster size = 3 so a triangle (not ever going to exceed 5)\n",
    "# 3 rectangles to fit within the first rectangle \n",
    "\n",
    "# in a 16:9\n",
    "\n",
    "# Cluster one in rect 1 (y = 16, 9/4) (w: 8, l: 9/2)\n",
    "# cluster Two in rect 2 (y = 16, 9/4*3) (w: 8, l: 9/2)\n",
    "# Cluster three in rect 3 (y = 8, 9/4) Biggest cluster ? (w: 8, l: 9/2)\n",
    "\n",
    "# Where to put the words \n",
    "# Start with the highest frequence with the biggest font : assign max font size before starting to draw\n",
    "# If the next one is smaller in frequence, font size drops by \n",
    "# font size 35 to 18\n",
    "# random choice where the word fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
