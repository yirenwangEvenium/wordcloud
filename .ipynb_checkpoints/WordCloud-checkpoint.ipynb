{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AffinityPropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from math import cos\n",
    "from math import sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answers to holidays\n",
    "docs = [\n",
    "    'I went to France to visit Paris',\n",
    "    'New York City was crowded',\n",
    "    'sightseeing in Bali',\n",
    "    'swimming in the tropical oceans',\n",
    "    'swimming in Bali',\n",
    "    'raining in Paris',\n",
    "    'visiting monuments in Paris',\n",
    "    'San Francisco',\n",
    "    'San Francisco',\n",
    "    'flew to Berlin',\n",
    "    'enjoyed visiting eiffel tower in paris',\n",
    "    'Paris, city of love',\n",
    "    'Flying is fun',\n",
    "    'Paris is crowded',\n",
    "    'France'\n",
    "]\n",
    "\n",
    "docs = [x.strip('\\n') for x in open('answers.txt').readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped docs [become vegetarian, use electric cars, use public transport, Make big companies pollute less, use solar, clean energy less meat less burger shorter shower bath dry toilet electric car, reduce carbon emissions, overconsumption planned obsolescence, Pollute less Renewable energy Better industies, ecology renewable energy, Cleean energies, Clean energies, bike work, pollute less, use reusable items, Power home renewable energy, Invest energy efficient appliances, Reduce water waste, eat food buy make less meat, fuel efficient vehicle, nuclear fusion wind turbines solar pannels tidal turbines education, green ecology electric solar wind nuclear, Shrink carbon profile, Sensibilisation research, Replace regular incandescent light bulb, Drive less Carpool, Reduce Reuse Recycle, Go Solar, buy energy efficient appliances, Reduce waste, Avoid products packaging, Turn lights, Turn Electronic Devices, Plant Tree, Use Clean Fuel, education, education, teach, electric cars, Renewable Fuel, Go Green, use renewable energies, eat less hamburger, nuclear fusion, renewable energies, ride bike, reuse towels, take lunch Tupperware, reduce fossil fuel use, Plant trees, Reduce Waste, Conserve water, Reduce Reuse Recycle, Use less heat Air Conditioning, Drive less Drive smart, buy energy efficient products, Plant Tree, use clean energy]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "def stop_word_stripper(line):\n",
    "    stop_words = [w.strip('\\n').lower() for w in open('stop_words.txt').readlines()]\n",
    "    pos_stopper = ['PUNCT', 'SYM']\n",
    "    return ' '.join([token.text for token in line if str(token).lower() not in stop_words and token.pos_  not in pos_stopper])\n",
    "\n",
    "stripped_docs = [] #spacy object\n",
    "copy_docs = [] # strings\n",
    "for d in docs:\n",
    "    stripped_docs.append(nlp(stop_word_stripper(nlp(d))))\n",
    "    copy_docs.append(stop_word_stripper(nlp(d)))\n",
    "    \n",
    "print('stripped docs', stripped_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1} ['become vegetarian', 'use electric cars', 'use public transport', 'Make big companies pollute less', 'use solar', 'clean energy less meat less burger shorter shower bath dry toilet electric car', 'reduce carbon emissions', 'overconsumption planned obsolescence', 'Pollute less Renewable energy Better industies', 'ecology renewable energy', 'Cleean energies', 'Clean energies', 'bike work', 'pollute less', 'use reusable items', 'Power home renewable energy', 'Invest energy efficient appliances', 'Reduce water waste', 'eat food buy make less meat', 'fuel efficient vehicle', 'nuclear fusion wind turbines solar pannels tidal turbines education', 'green ecology electric solar wind nuclear', 'Shrink carbon profile', 'Sensibilisation research', 'Replace regular incandescent light bulb', 'Drive less Carpool', 'Reduce Reuse Recycle', 'Go Solar', 'buy energy efficient appliances', 'Reduce waste', 'Avoid products packaging', 'Turn lights', 'Turn Electronic Devices', 'Plant Tree', 'Use Clean Fuel', 'education', 'education', 'teach', 'electric cars', 'Renewable Fuel', 'Go Green', 'use renewable energies', 'eat less hamburger', 'nuclear fusion', 'renewable energies', 'ride bike', 'reuse towels', 'take lunch', 'reduce fossil fuel use', 'Plant trees', 'Reduce Waste', 'Conserve water', 'Reduce Reuse Recycle', 'Use less heat', 'Drive less Drive smart', 'buy energy efficient products', 'Plant Tree', 'use clean energy']\n"
     ]
    }
   ],
   "source": [
    "# parse through to get entities \n",
    "kw_freq = {}\n",
    "\n",
    "for i in range(len(stripped_docs)):\n",
    "    line = stripped_docs[i]\n",
    "    for e in line.ents:\n",
    "        copy_docs[i] = copy_docs[i].replace(e.text, '').strip()\n",
    "        if e.text in kw_freq:\n",
    "            kw_freq[e.text] += 1\n",
    "        else:\n",
    "            kw_freq[e.text] = 1\n",
    "\n",
    "print(kw_freq, copy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1, 'become': 1, 'vegetarian': 1, 'electric': 4, 'public': 1, 'transport': 1, 'make': 2, 'company': 1, 'pollute': 3, 'less': 10, 'solar': 4, 'clean': 4, 'energy': 12, 'meat': 2, 'burger': 1, 'short': 1, 'shower': 1, 'bath': 1, 'toilet': 1, 'reduce': 7, 'carbon': 2, 'emission': 1, 'overconsumption': 1, 'plan': 1, 'obsolescence': 1, 'renewable': 6, 'better': 1, 'industie': 1, 'ecology': 2, 'cleean': 1, 'bike': 2, 'work': 1, 'reusable': 1, 'item': 1, 'power': 1, 'home': 1, 'invest': 1, 'efficient': 4, 'appliance': 2, 'water': 2, 'waste': 3, 'food': 1, 'fuel': 4, 'vehicle': 1, 'nuclear': 3, 'fusion': 2, 'wind': 2, 'turbine': 2, 'pannel': 1, 'tidal': 1, 'education': 3, 'green': 2, 'shrink': 1, 'profile': 1, 'sensibilisation': 1, 'research': 1, 'replace': 1, 'regular': 1, 'incandescent': 1, 'light': 2, 'bulb': 1, 'drive': 3, 'carpool': 1, 'reuse': 3, 'recycle': 2, 'avoid': 1, 'product': 2, 'packaging': 1, 'turn': 2, 'electronic': 1, 'devices': 1, 'plant': 3, 'tree': 3, 'teach': 1, 'hamburger': 1, 'ride': 1, 'towel': 1, 'take': 1, 'lunch': 1, 'fossil': 1, 'conserve': 1, 'heat': 1, 'smart': 1}\n"
     ]
    }
   ],
   "source": [
    "# get lemma keywords \n",
    "# join the rest of the words together: \n",
    "\n",
    "corpus = nlp(' '.join(copy_docs))\n",
    "\n",
    "MIN_CHARACTERS = 3\n",
    "\n",
    "for token in corpus:\n",
    "    if len(token.lemma_) > MIN_CHARACTERS:\n",
    "        if token.lemma_ in kw_freq:\n",
    "            kw_freq[token.lemma_] += 1\n",
    "        else:\n",
    "            kw_freq[token.lemma_] = 1\n",
    "\n",
    "print(kw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 2, 'Ear Conditioning': 1, 'become': 2, 'vegetarian': 2, 'public': 2, 'transport': 2, 'company': 2, 'burger': 2, 'Short': 1, 'shower': 2, 'bat': 1, 'toilet': 2, 'emission': 2, 'overconsumption': 2, 'plant': 4, 'obsolescence': 2, 'better': 2, 'industry': 1, 'clean': 5, 'wok': 1, 'reusable': 2, 'time': 1, 'powers': 1, 'hone': 1, 'incest': 1, 'food': 2, 'vehicle': 2, 'panel': 1, 'tidal': 2, 'shrunk': 1, 'profiles': 1, 'sensibility': 1, 'research': 2, 'replace': 2, 'regular': 2, 'incandescent': 2, 'bulb': 2, 'carpool': 2, 'avid': 1, 'packaging': 2, 'electronic': 2, 'devices': 2, 'teas': 1, 'hamburger': 2, 'ired': 1, 'rowel': 1, 'make': 3, 'lynch': 1, 'fossil': 2, 'conserve': 2, 'meat': 3, 'mart': 1}\n"
     ]
    }
   ],
   "source": [
    "# spellcheck keywords: \n",
    "from hunspell import Hunspell\n",
    "h = Hunspell();\n",
    "spell_checked_kw_freq = {}\n",
    "for kw, freq in kw_freq.items():\n",
    "    if freq == 1:\n",
    "        found = False\n",
    "        c_kws = h.suggest(kw) \n",
    "        for c in c_kws:\n",
    "            if c in kw_freq:\n",
    "                spell_checked_kw_freq[c] = kw_freq[c] + 1\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            spell_checked_kw_freq[c_kws[0]] = 1\n",
    "print(spell_checked_kw_freq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 1, 'Air Conditioning': 1, 'become': 1, 'vegetarian': 1, 'electric': 4, 'public': 1, 'transport': 1, 'make': 2, 'company': 1, 'pollute': 3, 'less': 10, 'solar': 4, 'clean': 4, 'energy': 12, 'meat': 2, 'burger': 1, 'short': 1, 'shower': 1, 'bath': 1, 'toilet': 1, 'reduce': 7, 'carbon': 2, 'emission': 1, 'overconsumption': 1, 'plan': 1, 'obsolescence': 1, 'renewable': 6, 'better': 1, 'industie': 1, 'ecology': 2, 'cleean': 1, 'bike': 2, 'work': 1, 'reusable': 1, 'item': 1, 'power': 1, 'home': 1, 'invest': 1, 'efficient': 4, 'appliance': 2, 'water': 2, 'waste': 3, 'food': 1, 'fuel': 4, 'vehicle': 1, 'nuclear': 3, 'fusion': 2, 'wind': 2, 'turbine': 2, 'pannel': 1, 'tidal': 1, 'education': 3, 'green': 2, 'shrink': 1, 'profile': 1, 'sensibilisation': 1, 'research': 1, 'replace': 1, 'regular': 1, 'incandescent': 1, 'light': 2, 'bulb': 1, 'drive': 3, 'carpool': 1, 'reuse': 3, 'recycle': 2, 'avoid': 1, 'product': 2, 'packaging': 1, 'turn': 2, 'electronic': 1, 'devices': 1, 'plant': 3, 'tree': 3, 'teach': 1, 'hamburger': 1, 'ride': 1, 'towel': 1, 'take': 1, 'lunch': 1, 'fossil': 1, 'conserve': 1, 'heat': 1, 'smart': 1}\n"
     ]
    }
   ],
   "source": [
    "# proper casing\n",
    "\n",
    "caseless_freq = {}\n",
    "propercase_freq = {}\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if kw in caseless_freq:\n",
    "        caseless_freq[kw.lower()].append(count)\n",
    "    else:\n",
    "        caseless_freq[kw.lower()] = [count]\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if count == max(caseless_freq[kw.lower()]):\n",
    "        propercase_freq[kw] = sum(caseless_freq[kw.lower()])\n",
    "\n",
    "print(propercase_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 300) ['Tupperware', 'Air Conditioning', 'become', 'vegetarian', 'electric', 'public', 'transport', 'make', 'company', 'pollute', 'less', 'solar', 'clean', 'energy', 'meat', 'burger', 'short', 'shower', 'bath', 'toilet', 'reduce', 'carbon', 'emission', 'overconsumption', 'plan', 'obsolescence', 'renewable', 'better', 'industie', 'ecology', 'cleean', 'bike', 'work', 'reusable', 'item', 'power', 'home', 'invest', 'efficient', 'appliance', 'water', 'waste', 'food', 'fuel', 'vehicle', 'nuclear', 'fusion', 'wind', 'turbine', 'pannel', 'tidal', 'education', 'green', 'shrink', 'profile', 'sensibilisation', 'research', 'replace', 'regular', 'incandescent', 'light', 'bulb', 'drive', 'carpool', 'reuse', 'recycle', 'avoid', 'product', 'packaging', 'turn', 'electronic', 'devices', 'plant', 'tree', 'teach', 'hamburger', 'ride', 'towel', 'take', 'lunch', 'fossil', 'conserve', 'heat', 'smart']\n"
     ]
    }
   ],
   "source": [
    "# semantic k means clustering\n",
    "\n",
    "glove_vectors = []\n",
    "labels_array = []\n",
    "\n",
    "for kw, count in propercase_freq.items():\n",
    "    labels_array.append(kw)\n",
    "    glove_vectors.append(nlp(kw)[0].vector)\n",
    "\n",
    "print(np.array(glove_vectors).shape, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Tupperware': 5, 'Air Conditioning': 5, 'become': 0, 'vegetarian': 2, 'electric': 1, 'public': 0, 'transport': 5, 'make': 0, 'company': 5, 'pollute': 5, 'less': 0, 'solar': 1, 'clean': 0, 'energy': 1, 'meat': 2, 'burger': 2, 'short': 0, 'shower': 3, 'bath': 3, 'toilet': 3, 'reduce': 0, 'carbon': 5, 'emission': 5, 'overconsumption': 5, 'plan': 0, 'obsolescence': 5, 'renewable': 1, 'better': 0, 'industie': 5, 'ecology': 5, 'cleean': 5, 'bike': 4, 'work': 0, 'reusable': 7, 'item': 5, 'power': 1, 'home': 0, 'invest': 0, 'efficient': 0, 'appliance': 5, 'water': 0, 'waste': 7, 'food': 0, 'fuel': 1, 'vehicle': 5, 'nuclear': 1, 'fusion': 5, 'wind': 1, 'turbine': 5, 'pannel': 5, 'tidal': 5, 'education': 5, 'green': 5, 'shrink': 5, 'profile': 5, 'sensibilisation': 5, 'research': 5, 'replace': 0, 'regular': 0, 'incandescent': 6, 'light': 0, 'bulb': 6, 'drive': 0, 'carpool': 5, 'reuse': 7, 'recycle': 7, 'avoid': 0, 'product': 5, 'packaging': 5, 'turn': 0, 'electronic': 5, 'devices': 5, 'plant': 5, 'tree': 5, 'teach': 0, 'hamburger': 2, 'ride': 4, 'towel': 3, 'take': 0, 'lunch': 2, 'fossil': 5, 'conserve': 7, 'heat': 1, 'smart': 0}\n"
     ]
    }
   ],
   "source": [
    "# AffinityPropagation clustering \n",
    "\n",
    "AffinityPropagation_model = AffinityPropagation()\n",
    "AffinityPropagation_model.fit(glove_vectors)\n",
    "\n",
    "cluster_labels    = AffinityPropagation_model.labels_\n",
    "\n",
    "clusters = {}\n",
    "kw_cluster = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "    kw_cluster[labels_array[i]] = cluster_labels[i]\n",
    "\n",
    "print (kw_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# k means clustering \n",
    "\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=4, n_init=5)\n",
    "kmeans_model.fit(glove_vectors)\n",
    "\n",
    "\n",
    "cluster_labels    = kmeans_model.labels_\n",
    "\n",
    "clusters = {}\n",
    "kw_cluster = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "    kw_cluster[labels_array[i]] = cluster_labels[i]\n",
    "\n",
    "print (kw_cluster)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: divide by zero encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:643: RuntimeWarning: overflow encountered in square\n",
      "  vv = np.average(np.square(v), weights=w)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:642: RuntimeWarning: overflow encountered in square\n",
      "  uu = np.average(np.square(u), weights=w)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:641: RuntimeWarning: overflow encountered in multiply\n",
      "  uv = np.average(u * v, weights=w)\n",
      "/usr/local/lib/python3.6/site-packages/scipy/spatial/distance.py:644: RuntimeWarning: invalid value encountered in float_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "#distance matrix (len(cluster_labels)^2)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "n = len(labels_array)\n",
    "\n",
    "distance_matrix = np.zeros([n, n])\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        distance_matrix[i][j] = spatial.distance.cosine(glove_vectors[i], glove_vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': 80, 'less': 70, 'reduce': 57, 'renewable': 52, 'electric': 43, 'solar': 43, 'clean': 43, 'efficient': 43, 'fuel': 43, 'pollute': 39, 'waste': 39, 'nuclear': 39, 'education': 39, 'drive': 39, 'reuse': 39, 'plant': 39, 'tree': 39, 'make': 34, 'meat': 34, 'carbon': 34, 'ecology': 34, 'bike': 34, 'appliance': 34, 'water': 34, 'fusion': 34, 'wind': 34, 'turbine': 34, 'green': 34, 'light': 34, 'recycle': 34, 'product': 34, 'turn': 34, 'Tupperware': 30, 'Air Conditioning': 30, 'become': 30, 'vegetarian': 30, 'public': 30, 'transport': 30, 'company': 30, 'burger': 30, 'short': 30, 'shower': 30, 'bath': 30, 'toilet': 30, 'emission': 30, 'overconsumption': 30, 'plan': 30, 'obsolescence': 30, 'better': 30, 'industie': 30, 'cleean': 30, 'work': 30, 'reusable': 30, 'item': 30, 'power': 30, 'home': 30, 'invest': 30, 'food': 30, 'vehicle': 30, 'pannel': 30, 'tidal': 30, 'shrink': 30, 'profile': 30, 'sensibilisation': 30, 'research': 30, 'replace': 30, 'regular': 30, 'incandescent': 30, 'bulb': 30, 'carpool': 30, 'avoid': 30, 'packaging': 30, 'electronic': 30, 'devices': 30, 'teach': 30, 'hamburger': 30, 'ride': 30, 'towel': 30, 'take': 30, 'lunch': 30, 'fossil': 30, 'conserve': 30, 'heat': 30, 'smart': 30}\n"
     ]
    }
   ],
   "source": [
    "# assign max font size\n",
    "\n",
    "def assign_font_size(propercase_freq, max_size, min_size):\n",
    "    label_fs = {}\n",
    "    sorted_tuples = [(k, propercase_freq[k]) for k in sorted(propercase_freq, key=propercase_freq.get, reverse=True)]\n",
    "    min_count = sorted_tuples[-1][1]\n",
    "    max_count = sorted_tuples[0][1]\n",
    "    \n",
    "    for kw, count in sorted_tuples:\n",
    "        if (max_count - min_count) == 0:\n",
    "            size = int((max_size - min_size) / 2.0 + min_size)\n",
    "        else:\n",
    "            #size = int(min_size + (max_size - min_size) * (count * 1.0 / (max_count - min_count)) ** 0.8)\n",
    "            size = int((max_size - min_size)/(max_count - min_count)*count + min_size - (max_size - min_size)/(max_count - min_count)*min_count)\n",
    "        label_fs[kw] = size\n",
    "    \n",
    "    return (label_fs)\n",
    "        \n",
    "kw_fs = assign_font_size(propercase_freq, 80, 30) #keyword_font_size\n",
    "print(kw_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': (335, 80), 'less': (196, 70), 'reduce': (239, 57), 'renewable': (327, 52), 'electric': (240, 43), 'solar': (150, 43), 'clean': (150, 43), 'efficient': (270, 43), 'fuel': (120, 43), 'pollute': (191, 39), 'waste': (136, 39), 'nuclear': (191, 39), 'education': (245, 39), 'drive': (136, 39), 'reuse': (136, 39), 'plant': (136, 39), 'tree': (109, 39), 'make': (95, 34), 'meat': (95, 34), 'carbon': (142, 34), 'ecology': (166, 34), 'bike': (95, 34), 'appliance': (214, 34), 'water': (119, 34), 'fusion': (142, 34), 'wind': (95, 34), 'turbine': (166, 34), 'green': (119, 34), 'light': (119, 34), 'recycle': (166, 34), 'product': (166, 34), 'turn': (95, 34), 'Tupperware': (210, 30), 'Air Conditioning': (336, 30), 'become': (125, 30), 'vegetarian': (210, 30), 'public': (125, 30), 'transport': (189, 30), 'company': (146, 30), 'burger': (125, 30), 'short': (105, 30), 'shower': (125, 30), 'bath': (84, 30), 'toilet': (125, 30), 'emission': (168, 30), 'overconsumption': (315, 30), 'plan': (84, 30), 'obsolescence': (251, 30), 'better': (125, 30), 'industie': (168, 30), 'cleean': (125, 30), 'work': (84, 30), 'reusable': (168, 30), 'item': (84, 30), 'power': (105, 30), 'home': (84, 30), 'invest': (125, 30), 'food': (84, 30), 'vehicle': (146, 30), 'pannel': (125, 30), 'tidal': (105, 30), 'shrink': (125, 30), 'profile': (146, 30), 'sensibilisation': (315, 30), 'research': (168, 30), 'replace': (146, 30), 'regular': (146, 30), 'incandescent': (251, 30), 'bulb': (84, 30), 'carpool': (146, 30), 'avoid': (105, 30), 'packaging': (189, 30), 'electronic': (210, 30), 'devices': (146, 30), 'teach': (105, 30), 'hamburger': (189, 30), 'ride': (84, 30), 'towel': (105, 30), 'take': (84, 30), 'lunch': (105, 30), 'fossil': (125, 30), 'conserve': (168, 30), 'heat': (84, 30), 'smart': (105, 30)}\n"
     ]
    }
   ],
   "source": [
    "def max_dimensions(kw_fs):\n",
    "    kw_dimensions = {}\n",
    "    for kw, fs in kw_fs.items():\n",
    "        kw_dimensions[kw] = (int(0.7*len(kw)*fs), fs) #x, y (i.e. width, height)\n",
    "    return kw_dimensions\n",
    "\n",
    "kw_max_dim = max_dimensions(kw_fs)\n",
    "print(kw_max_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nr1 = {\\n    \"x\" : 0,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\n\\nr2 = {\\n    \"x\" : 5,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\nrect_intersection(r1, r2)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you need x,y width height for each rectangle (word)\n",
    "# r1 x, y, width, height \n",
    "# p1--------\n",
    "#  |        |\n",
    "#  |        |\n",
    "#  |--------p2\n",
    "\n",
    "def rect_intersection(r1, r2): #return true if intersects\n",
    "    p1 = {}\n",
    "    p1[\"x\"] = r1[\"x\"]\n",
    "    p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "    \n",
    "    p2 = {}\n",
    "    p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "    p2[\"y\"] = r1[\"y\"]\n",
    "    \n",
    "    p3 = {}\n",
    "    p3[\"x\"] = r2[\"x\"]\n",
    "    p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "    \n",
    "    p4 = {}\n",
    "    p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "    p4[\"y\"] = r2[\"y\"]\n",
    "    \n",
    "    return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "#test\n",
    "'''\n",
    "r1 = {\n",
    "    \"x\" : 0,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "\n",
    "r2 = {\n",
    "    \"x\" : 5,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "rect_intersection(r1, r2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, size, font_size, cluster):\n",
    "        self.word = word\n",
    "        self.width = size[\"width\"] #{width, height}\n",
    "        self.height = size[\"height\"]\n",
    "        self.font_size = font_size\n",
    "        self.cluster = cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cloud:\n",
    "    def __init__(self, words=[], canvas_size={\"x\": 1920, \"y\": 1080}, filename='clouds.html'):\n",
    "        self.words = words\n",
    "        self.canvas = [] #{word, font_size, x, y, width, height, color, cluster} <== color to be added\n",
    "        self.canvas_size = canvas_size\n",
    "        self.clusters = self.generate_clusters() # {0 : cluster0, 1 : cluster1, ...etc}\n",
    "        self.filename = filename\n",
    "        self.colors = [\"#5F0B2B\",\"#D11638\", \"#F08801\", \"#FACE00\", \"#ADA20B\", \"#D4D639\", \"#EF6D3E\", \"#90A6BF\", \"#F2AEB4\", \"#AA61AE\", \"#FBC6CE\"]\n",
    "        self.positions = []\n",
    "        \n",
    "    def generate_clusters(self):\n",
    "        clusters = {}\n",
    "        for w in self.words:\n",
    "            if w.cluster in clusters:\n",
    "                clusters[w.cluster].append(w)\n",
    "            else:\n",
    "                clusters[w.cluster] = [w]\n",
    "        return clusters\n",
    "    \n",
    "    '''\n",
    "    def choose_cluster_start(self):\n",
    "        start_points = {}\n",
    "        start_point = {}\n",
    "        r = 0\n",
    "        for i in range(len(self.clusters)):\n",
    "            c = self.clusters[i]\n",
    "            n = len(c)\n",
    "            \n",
    "            H = self.canvas_size[\"y\"] #total height\n",
    "            L = self.canvas_size[\"x\"] #total length\n",
    "            \n",
    "            if i%2 == 0:\n",
    "                y = random.randint(int(0.1*H), int(0.55*H))\n",
    "            else:\n",
    "                y = random.randint(int(0.55*H), int(0.9*H))\n",
    "            x = random.randint(int(r*L), min(int((r+len(c)/len(self.words))*L), int(L*0.90)))\n",
    "            \n",
    "            r = min(0.85, r + len(c)/len(self.words))\n",
    "            start_points[c[0].cluster] = {\n",
    "                \"x\": x,\n",
    "                \"y\": y\n",
    "            }\n",
    "        return start_points\n",
    "    '''\n",
    "        \n",
    "    def create_cloud_svg(self):\n",
    "        cl_size = {}\n",
    "        for c, words in self.clusters.items():\n",
    "            cl_size[c] = len(words)\n",
    "        sorted_clusters = sorted(cl_size, key=cl_size.get)[::-1]\n",
    "        \n",
    "        start_position = { \"x\": 1920//2, \"y\": 1080//3 }\n",
    "        \n",
    "        for c in sorted_clusters:\n",
    "            words = self.clusters[c]\n",
    "            self.positions = self.spiral(start_position)\n",
    "            for w in words:\n",
    "                new_position = self.add_word_to_cloud(w) \n",
    "                \n",
    "            start_position = new_position\n",
    "            \n",
    "        f = open(self.filename, 'w')\n",
    "        f.write('<svg viewbox=\"0 0 1920 1080\">')\n",
    "        for w in cloud.canvas:\n",
    "            f.write('<text x=\"{}\" y=\"{}\" font-family=\"Verdana\" font-size=\"{}\" stroke=\"none\" fill=\"{}\">'.format(w[\"x\"], w[\"y\"], w[\"font_size\"], w[\"color\"]))\n",
    "            f.write(w[\"word\"])\n",
    "            f.write('</text>\\n')\n",
    "        f.write('</svg>')\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    def add_word_to_cloud(self, word): # word class Word        \n",
    "            \n",
    "        \n",
    "        for p in self.positions:\n",
    "            if self.verify_overlap( word, p):\n",
    "                self.positions.remove(p)\n",
    "            else:\n",
    "                self.canvas.append({\n",
    "                    \"word\": word.word,\n",
    "                    \"x\": p[\"x\"],\n",
    "                    \"y\": p[\"y\"],\n",
    "                    \"width\": word.width,\n",
    "                    \"height\": word.height,\n",
    "                    \"font_size\": word.font_size,\n",
    "                    \"color\": self.colors[word.cluster],\n",
    "                    \"cluster\": word.cluster\n",
    "                })\n",
    "                self.positions.remove(p)\n",
    "                return p\n",
    "            \n",
    "\n",
    "    def rect_intersection(self, r1, r2):\n",
    "        p1 = {}\n",
    "        p1[\"x\"] = r1[\"x\"]\n",
    "        p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "\n",
    "        p2 = {}\n",
    "        p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "        p2[\"y\"] = r1[\"y\"]\n",
    "\n",
    "        p3 = {}\n",
    "        p3[\"x\"] = r2[\"x\"]\n",
    "        p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "\n",
    "        p4 = {}\n",
    "        p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "        p4[\"y\"] = r2[\"y\"]\n",
    "\n",
    "        return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "    \n",
    "    def verify_overlap(self, word, position): # true if overlaps, false if not\n",
    "        new_rect = {\n",
    "            \"x\": position[\"x\"],\n",
    "            \"y\": position[\"y\"],\n",
    "            \"width\": word.width,\n",
    "            \"height\": word.height\n",
    "        }\n",
    "        for filled_rect in self.canvas:\n",
    "            if self.rect_intersection(filled_rect, new_rect):\n",
    "                return True\n",
    "        #verify out of bound of rectangle:\n",
    "        if new_rect[\"x\"] < 0 or new_rect[\"x\"] + new_rect[\"width\"] > 1920 or new_rect[\"y\"] > 1080 or new_rect[\"y\"]- new_rect[\"height\"] < 0:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def spiral(self, start_point): # returns an [] with positions to test \n",
    "        points = [start_point]\n",
    "        # x = (a + b*theta)cos(theta)\n",
    "        # y = (a + b*theta)sin(theta)\n",
    "\n",
    "        # b = a final - a ini / 2 pi n  n=number of turns\n",
    "        a_ini = 0\n",
    "        # a_final = self.canvas_size[\"x\"]*len(self.clusters[cluster])/len(self.words) #spiral radius \n",
    "        a_final = self.canvas_size[\"x\"] #spiral radius \n",
    "\n",
    "        b = (a_final - a_ini)/(2*3.14159*(self.canvas_size[\"y\"]/10))\n",
    "\n",
    "        thetas = [ (self.canvas_size[\"y\"]/10 * 2)/1000 *x for x in range(1000)]\n",
    "        for i in thetas: #1000 points\n",
    "            x = ( a_ini + b*i )*cos(i) + start_point[\"x\"]\n",
    "            y = ( a_ini + b*i )*sin(i) + start_point[\"y\"]\n",
    "            points.append({\"x\": x, \"y\": y})\n",
    "\n",
    "        return points\n",
    "    \n",
    "    '''\n",
    "    def compress(self):\n",
    "        # pull words towards the one zith the most occurence\n",
    "        # create line \n",
    "        # test positions along that line \n",
    "        sizes = []\n",
    "        for w in self.canvas:\n",
    "            sizes.append(w[\"font_size\"])\n",
    "        central_word = self.canvas[sizes.index(max(sizes))]\n",
    "        \n",
    "        for w in self.canvas:\n",
    "            if w[\"cluster\"] != central_word[\"cluster\"]:\n",
    "                # sort tham by distance \n",
    "                pos_central_word = np.array([central_word[\"x\"], central_word[\"y\"]])\n",
    "                pos_w = np.array([w[\"x\"], w[\"y\"]])\n",
    "                dist = numpy.sqrt(numpy.sum((pos_central_word - pos_w)**2))\n",
    "                \n",
    "                # draw line \n",
    "                # inch closer \n",
    "                coeff = central_word[\"y\"] - w[\"y\"] / central_word[\"x\"] - w[\"x\"]\n",
    "                coordiantes = [{\"x\": central_word[\"x\"] + (central_word[\"x\"] - w[\"x\"])/100 * i, \"y\": central_word[\"y\"] + coeff* (central_word[\"x\"] - w[\"x\"])/100 * i } for i in range(100)]\n",
    "                for c in coordinates:\n",
    "                    for word in self.words:\n",
    "                        if self.verify_overlap(word, c):\n",
    "                            break\n",
    "    '''             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport matplotlib.pyplot as plt\\n\\nfig = plt.figure(figsize=(10,10))\\n\\nax = fig.add_subplot(111)\\nfig.subplots_adjust(top=0.85)\\n\\nfor w in cloud.canvas:\\n    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\\n\\nax.axis([0, 1920, 0, 1080])\\n\\n'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for kw, d in kw_max_dim.items():\n",
    "    words.append(Word(kw, {\"width\": d[0], \"height\": d[1]}, kw_fs[kw], kw_cluster[kw]))\n",
    "\n",
    "cloud = Cloud(words=words)\n",
    "\n",
    "cloud.create_cloud_svg()\n",
    "\n",
    "#cloud.compress()\n",
    "\n",
    "#print(cloud.canvas)\n",
    "'''\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "for w in cloud.canvas:\n",
    "    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\n",
    "\n",
    "ax.axis([0, 1920, 0, 1080])\n",
    "\n",
    "'''\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    def __init__(self, node):\n",
    "        self.id = node # we have a dict {id : { word, weight } }\n",
    "        self.adjacent = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])\n",
    "\n",
    "    def add_neighbor(self, neighbor, weight=0):\n",
    "        self.adjacent[neighbor] = weight\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.adjacent.keys()  \n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, neighbor):\n",
    "        return self.adjacent[neighbor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.vert_dict = {}\n",
    "        self.num_vertices = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_dict.values())\n",
    "\n",
    "    def add_vertex(self, node):\n",
    "        self.num_vertices = self.num_vertices + 1\n",
    "        new_vertex = Vertex(node)\n",
    "        self.vert_dict[node] = new_vertex\n",
    "        return new_vertex\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_dict:\n",
    "            return self.vert_dict[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_edge(self, frm, to, cost = 0):\n",
    "        if frm not in self.vert_dict:\n",
    "            self.add_vertex(frm)\n",
    "        if to not in self.vert_dict:\n",
    "            self.add_vertex(to)\n",
    "\n",
    "        self.vert_dict[frm].add_neighbor(self.vert_dict[to], cost)\n",
    "        self.vert_dict[to].add_neighbor(self.vert_dict[frm], cost)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_dict.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Star Forest Clustering and putting together\n",
    "'''\n",
    "\n",
    "def StarForestAlgo(g):\n",
    "    '''\n",
    "    g = similarity graph\n",
    "    '''\n",
    "    stars = []\n",
    "    while True:\n",
    "        usedVertices = []\n",
    "        bestCenter = findBestCenter(g, usedVertices)\n",
    "\n",
    "        if bestCenter is None:\n",
    "            break; \n",
    "        \n",
    "        star, usedVertices = createGraphStar(g, bestCenter, usedVertices) # graph, vertice, [vertices]\n",
    "        print(usedVertices)\n",
    "        stars.append(star)\n",
    "        \n",
    "        \n",
    "    return stars\n",
    "\n",
    "\n",
    "def findBestCenter(g, usedVertices): # graph, [vertices]\n",
    "    best_sum = 0\n",
    "    best_center = None\n",
    "    for v in g.get_vertices():\n",
    "        if v not in usedVertices:\n",
    "            sum = getSumOfConnectedEdges(g, v, usedVertices)\n",
    "            if sum > best_sum:\n",
    "                best_center = v\n",
    "    return best_center\n",
    "\n",
    "\n",
    "def getSumOfConnectedEdges(g, v, usedVertices):\n",
    "    sum = 0\n",
    "    connections = g.get_vertex(v).get_connections()\n",
    "    for c in connections:\n",
    "        if c not in usedVertices:\n",
    "            sum += g.get_vertex(v).get_weight(c)\n",
    "    return sum\n",
    "    \n",
    "\n",
    "def createGraphStar(g, bestCenter, usedVertices):\n",
    "    star = Graph()\n",
    "    for v in g.get_vertex(bestCenter).get_connections():\n",
    "        if v not in usedVertices and g.get_vertex(bestCenter) != v:\n",
    "            star.add_edge(bestCenter, v, g.get_vertex(bestCenter).get_weight(v))\n",
    "            print(v)\n",
    "            usedVertices.append(v)\n",
    "    return star, usedVertices\n",
    "\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.add_vertex('a')\n",
    "g.add_vertex('b')\n",
    "g.add_vertex('c')\n",
    "g.add_vertex('d')\n",
    "g.add_vertex('e')\n",
    "g.add_vertex('f')\n",
    "\n",
    "g.add_edge('a', 'b', 7)  \n",
    "g.add_edge('a', 'c', 9)\n",
    "g.add_edge('a', 'f', 14)\n",
    "g.add_edge('b', 'c', 10)\n",
    "g.add_edge('b', 'd', 15)\n",
    "g.add_edge('c', 'd', 11)\n",
    "g.add_edge('c', 'f', 2)\n",
    "g.add_edge('d', 'e', 6)\n",
    "g.add_edge('e', 'f', 9)\n",
    "\n",
    "#StarForestAlgo(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aspect ration of words : font_size (length) font_size*0.7(width)\n",
    "# Aspect ration of SVG file is 16:9\n",
    "\n",
    "# How to draw V1 \n",
    "# Create a polygon with the number of vertices = number of clusters \n",
    "# here cluster size = 3 so a triangle (not ever going to exceed 5)\n",
    "# 3 rectangles to fit within the first rectangle \n",
    "\n",
    "# in a 16:9\n",
    "\n",
    "# Cluster one in rect 1 (y = 16, 9/4) (w: 8, l: 9/2)\n",
    "# cluster Two in rect 2 (y = 16, 9/4*3) (w: 8, l: 9/2)\n",
    "# Cluster three in rect 3 (y = 8, 9/4) Biggest cluster ? (w: 8, l: 9/2)\n",
    "\n",
    "# Where to put the words \n",
    "# Start with the highest frequence with the biggest font : assign max font size before starting to draw\n",
    "# If the next one is smaller in frequence, font size drops by \n",
    "# font size 35 to 18\n",
    "# random choice where the word fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    def seam_carving(self):\n",
    "        board = self.make_board()\n",
    "        sparse = 0\n",
    "        while sparse < 500:\n",
    "            print(sparse)\n",
    "            sparse += 1\n",
    "            self.find_and_remove_path(board)\n",
    "            board = self.make_board()\n",
    "            \n",
    "    def find_and_remove_path(self, board):\n",
    "        v_path = []\n",
    "        h_path = []\n",
    "        sparse = True\n",
    "        for y in range(self.canvas_size[\"y\"]):\n",
    "            pt = {}\n",
    "            pt[\"y\"] = y\n",
    "            pt[\"x\"] = np.argmin(board[y])\n",
    "            v_path.append(pt) # first step \n",
    "            \n",
    "        for x in range(self.canvas_size[\"x\"]):\n",
    "            pt = {}\n",
    "            pt[\"x\"] = x\n",
    "            pt[\"y\"] = np.argmin(board[:,x])\n",
    "            h_path.append(pt) # first step \n",
    "        \n",
    "        for p in v_path + h_path:\n",
    "            for w in self.canvas:\n",
    "                if w[\"x\"] > p[\"x\"]:\n",
    "                    w[\"x\"]-= 1\n",
    "                if w[\"y\"] > p[\"y\"]:\n",
    "                    w[\"y\"] -= 1\n",
    "                    \n",
    "        board = self.make_board()\n",
    "        return sparse\n",
    "\n",
    "    \n",
    "    def make_board(self):\n",
    "        cv = self.canvas\n",
    "        # map canvas to a 1920 1080 matrix \n",
    "        board = np.zeros(shape=(1080, 1920)) #lines, columns\n",
    "        for w in cv:\n",
    "            for i in range(int(w[\"y\"]) - int(w[\"height\"]), int(w[\"y\"])+1):\n",
    "                board[i][int(w[\"x\"]) : int(w[\"x\"]) + int(w[\"width\"]) +1 ] = 1\n",
    "        return board\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
