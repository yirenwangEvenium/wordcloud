{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from math import cos\n",
    "from math import sin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# answers to holidays\n",
    "docs = [\n",
    "    'I went to France to visit Paris',\n",
    "    'New York City was crowded',\n",
    "    'sightseeing in Bali',\n",
    "    'swimming in the tropical oceans',\n",
    "    'swimming in Bali',\n",
    "    'raining in Paris',\n",
    "    'visiting monuments in Paris',\n",
    "    'San Francisco',\n",
    "    'San Francisco',\n",
    "    'flew to Berlin',\n",
    "    'enjoyed visiting eiffel tower in paris',\n",
    "    'Paris, city of love',\n",
    "    'Flying is fun',\n",
    "    'Paris is crowded',\n",
    "    'France'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stripped docs [went France visit Paris, New York City crowded, sightseeing Bali, swimming tropical oceans, swimming Bali, raining Paris, visiting monuments Paris, San Francisco, San Francisco, flew Berlin, enjoyed visiting eiffel tower paris, Paris city love, Flying fun, Paris crowded, France]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "\n",
    "def stop_word_stripper(line):\n",
    "    stop_words = [w.strip('\\n').lower() for w in open('stop_words.txt').readlines()]\n",
    "    pos_stopper = ['PUNCT', 'SYM']\n",
    "    return ' '.join([token.text for token in line if str(token).lower() not in stop_words and token.pos_  not in pos_stopper])\n",
    "\n",
    "stripped_docs = [] #spacy object\n",
    "copy_docs = [] # strings\n",
    "for d in docs:\n",
    "    stripped_docs.append(nlp(stop_word_stripper(nlp(d))))\n",
    "    copy_docs.append(stop_word_stripper(nlp(d)))\n",
    "    \n",
    "print('stripped docs', stripped_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 5, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1} ['went  visit', 'crowded', 'sightseeing', 'swimming tropical oceans', 'swimming', 'raining', 'visiting monuments', '', '', 'flew', 'enjoyed visiting eiffel tower paris', 'city love', 'Flying fun', 'crowded', '']\n"
     ]
    }
   ],
   "source": [
    "# parse through to get entities \n",
    "kw_freq = {}\n",
    "\n",
    "for i in range(len(stripped_docs)):\n",
    "    line = stripped_docs[i]\n",
    "    for e in line.ents:\n",
    "        copy_docs[i] = copy_docs[i].replace(e.text, '').strip()\n",
    "        if e.text in kw_freq:\n",
    "            kw_freq[e.text] += 1\n",
    "        else:\n",
    "            kw_freq[e.text] = 1\n",
    "\n",
    "print(kw_freq, copy_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 5, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1, 'visit': 3, 'crowded': 1, 'sightseeing': 1, 'swim': 2, 'tropical': 1, 'ocean': 1, 'rain': 1, 'monument': 1, 'enjoy': 1, 'eiffel': 1, 'tower': 1, 'paris': 1, 'city': 1, 'love': 1, 'flying': 1, 'crowd': 1}\n"
     ]
    }
   ],
   "source": [
    "# get lemma keywords \n",
    "# join the rest of the words together: \n",
    "\n",
    "corpus = nlp(' '.join(copy_docs))\n",
    "\n",
    "MIN_CHARACTERS = 3\n",
    "\n",
    "for token in corpus:\n",
    "    if len(token.lemma_) > MIN_CHARACTERS:\n",
    "        if token.lemma_ in kw_freq:\n",
    "            kw_freq[token.lemma_] += 1\n",
    "        else:\n",
    "            kw_freq[token.lemma_] = 1\n",
    "\n",
    "print(kw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'France': 2, 'Paris': 6, 'New York City': 1, 'Bali': 2, 'San Francisco': 2, 'Berlin': 1, 'visit': 3, 'crowded': 1, 'sightseeing': 1, 'swim': 2, 'tropical': 1, 'ocean': 1, 'rain': 1, 'monument': 1, 'enjoy': 1, 'eiffel': 1, 'tower': 1, 'city': 1, 'love': 1, 'flying': 1, 'crowd': 1}\n"
     ]
    }
   ],
   "source": [
    "# proper casing\n",
    "\n",
    "caseless_freq = {}\n",
    "propercase_freq = {}\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if kw in caseless_freq:\n",
    "        caseless_freq[kw.lower()].append(count)\n",
    "    else:\n",
    "        caseless_freq[kw.lower()] = [count]\n",
    "\n",
    "for kw, count in kw_freq.items():\n",
    "    if count == max(caseless_freq[kw.lower()]):\n",
    "        propercase_freq[kw] = sum(caseless_freq[kw.lower()])\n",
    "\n",
    "print(propercase_freq)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 300) ['France', 'Paris', 'New York City', 'Bali', 'San Francisco', 'Berlin', 'visit', 'crowded', 'sightseeing', 'swim', 'tropical', 'ocean', 'rain', 'monument', 'enjoy', 'eiffel', 'tower', 'city', 'love', 'flying', 'crowd']\n"
     ]
    }
   ],
   "source": [
    "# semantic k means clustering\n",
    "\n",
    "glove_vectors = []\n",
    "labels_array = []\n",
    "\n",
    "for kw, count in propercase_freq.items():\n",
    "    labels_array.append(kw)\n",
    "    glove_vectors.append(nlp(kw)[0].vector)\n",
    "\n",
    "print(np.array(glove_vectors).shape, labels_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=5, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# k means clustering \n",
    "\n",
    "kmeans_model = KMeans(init='k-means++', n_clusters=len(labels_array)//5, n_init=5)\n",
    "kmeans_model.fit(glove_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 0 0 0 1 1 1 2 2 2 2 3 1 0 3 1 1 2 1]\n",
      "{0: ['France', 'Paris', 'Bali', 'San Francisco', 'Berlin', 'eiffel'], 1: ['New York City', 'visit', 'crowded', 'sightseeing', 'enjoy', 'city', 'love', 'crowd'], 2: ['swim', 'tropical', 'ocean', 'rain', 'flying'], 3: ['monument', 'tower']}\n"
     ]
    }
   ],
   "source": [
    "cluster_labels    = kmeans_model.labels_\n",
    "cluster_inertia   = kmeans_model.inertia_\n",
    "\n",
    "print(cluster_labels)\n",
    "\n",
    "clusters = {}\n",
    "for i in range(len(labels_array)):\n",
    "    if cluster_labels[i] not in clusters:\n",
    "        clusters[cluster_labels[i]] = [labels_array[i]]\n",
    "    else:\n",
    "        clusters[cluster_labels[i]].append(labels_array[i])\n",
    "\n",
    "print (clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#distance matrix (len(cluster_labels)^2)\n",
    "\n",
    "from scipy import spatial\n",
    "\n",
    "n = len(labels_array)\n",
    "\n",
    "distance_matrix = np.zeros([n, n])\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(n):\n",
    "        distance_matrix[i][j] = spatial.distance.cosine(glove_vectors[i], glove_vectors[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-79-5382ca94dca3>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-79-5382ca94dca3>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    label_fs[kw] = size\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# assign max font size\n",
    "\n",
    "def assign_font_size(propercase_freq, max_size, min_size):\n",
    "    label_fs = {}\n",
    "    sorted_tuples = [(k, propercase_freq[k]) for k in sorted(propercase_freq, key=propercase_freq.get, reverse=True)]\n",
    "    min_count = sorted_tuples[-1][1]\n",
    "    max_count = sorted_tuples[0][1]\n",
    "    \n",
    "    for kw, count in sorted_tuples:\n",
    "        if (max_count - min_count) == 0:\n",
    "            size = int((max_size - min_size) / 2.0 + min_size)\n",
    "        else:\n",
    "            #size = int(min_size + (max_size - min_size) * (count * 1.0 / (max_count - min_count)) ** 0.8)\n",
    "            size = int((max_size - min_size)/(max_count - min_count)*count + min_size - (max_size - min_size)/(max_count - min_count)*min_count)\n",
    "        label_fs[kw] = size\n",
    "    \n",
    "    return (label_fs)\n",
    "        \n",
    "kw_fs = assign_font_size(propercase_freq, 40, 18) #keyword_font_size\n",
    "print(kw_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Paris': (150, 43), 'visit': (112, 32), 'France': (117, 28), 'Bali': (78, 28), 'San Francisco': (254, 28), 'swim': (78, 28), 'New York City': (218, 24), 'Berlin': (100, 24), 'crowded': (117, 24), 'sightseeing': (184, 24), 'tropical': (134, 24), 'ocean': (84, 24), 'rain': (67, 24), 'monument': (134, 24), 'enjoy': (84, 24), 'eiffel': (100, 24), 'tower': (84, 24), 'city': (67, 24), 'love': (67, 24), 'flying': (100, 24), 'crowd': (84, 24)}\n"
     ]
    }
   ],
   "source": [
    "def max_dimensions(kw_fs):\n",
    "    kw_dimensions = {}\n",
    "    for kw, fs in kw_fs.items():\n",
    "        kw_dimensions[kw] = (int(0.7*len(kw)*fs), fs) #x, y (i.e. width, height)\n",
    "    return kw_dimensions\n",
    "\n",
    "kw_max_dim = max_dimensions(kw_fs)\n",
    "print(kw_max_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nr1 = {\\n    \"x\" : 0,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\n\\nr2 = {\\n    \"x\" : 5,\\n    \"y\" : 10,\\n    \"height\" : 10,\\n    \"width\" : 10\\n}\\n\\nrect_intersection(r1, r2)\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you need x,y width height for each rectangle (word)\n",
    "# r1 x, y, width, height \n",
    "# p1--------\n",
    "#  |        |\n",
    "#  |        |\n",
    "#  |--------p2\n",
    "\n",
    "def rect_intersection(r1, r2): #return true if intersects\n",
    "    p1 = {}\n",
    "    p1[\"x\"] = r1[\"x\"]\n",
    "    p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "    \n",
    "    p2 = {}\n",
    "    p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "    p2[\"y\"] = r1[\"y\"]\n",
    "    \n",
    "    p3 = {}\n",
    "    p3[\"x\"] = r2[\"x\"]\n",
    "    p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "    \n",
    "    p4 = {}\n",
    "    p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "    p4[\"y\"] = r2[\"y\"]\n",
    "    \n",
    "    return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "#test\n",
    "'''\n",
    "r1 = {\n",
    "    \"x\" : 0,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "\n",
    "r2 = {\n",
    "    \"x\" : 5,\n",
    "    \"y\" : 10,\n",
    "    \"height\" : 10,\n",
    "    \"width\" : 10\n",
    "}\n",
    "\n",
    "rect_intersection(r1, r2)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Word:\n",
    "    def __init__(self, word, size, font_size):\n",
    "        self.word = word\n",
    "        self.width = size[\"width\"] #{width, height}\n",
    "        self.height = size[\"height\"]\n",
    "        self.font_size = font_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cloud:\n",
    "    def __init__(self, words=[], aspect_ratio = '19:6', canvas_size={\"x\": 1920, \"y\": 1080}):\n",
    "        self.words = words\n",
    "        self.aspect_ratio = aspect_ratio\n",
    "        self.canvas = [] #{x, y, width, height}\n",
    "        self.canvas_size = canvas_size\n",
    "        self.possible_positions = self.spiral()\n",
    "        \n",
    "        \n",
    "    def add_word(self, word): # word class Word\n",
    "        for p in self.possible_positions:\n",
    "            if self.verify_overlap( word, p):\n",
    "                self.possible_positions.remove(p)\n",
    "            else:\n",
    "                self.words.append(word)\n",
    "                self.canvas.append({\n",
    "                    \"word\": word.word,\n",
    "                    \"x\": p[\"x\"],\n",
    "                    \"y\": p[\"y\"],\n",
    "                    \"width\": word.width,\n",
    "                    \"height\": word.height,\n",
    "                    \"font_size\": word.font_size\n",
    "                })\n",
    "                self.possible_positions.remove(p)\n",
    "                break;\n",
    "\n",
    "\n",
    "    def rect_intersection(self, r1, r2):\n",
    "        p1 = {}\n",
    "        p1[\"x\"] = r1[\"x\"]\n",
    "        p1[\"y\"] = r1[\"y\"] - r1[\"height\"]\n",
    "\n",
    "        p2 = {}\n",
    "        p2[\"x\"] = r1[\"x\"] + r1[\"width\"]\n",
    "        p2[\"y\"] = r1[\"y\"]\n",
    "\n",
    "        p3 = {}\n",
    "        p3[\"x\"] = r2[\"x\"]\n",
    "        p3[\"y\"] = r2[\"y\"] - r2[\"height\"]\n",
    "\n",
    "        p4 = {}\n",
    "        p4[\"x\"] = r2[\"x\"] + r2[\"width\"]\n",
    "        p4[\"y\"] = r2[\"y\"]\n",
    "\n",
    "        return not(p2[\"y\"] < p3[\"y\"] or p1[\"y\"] > p4[\"y\"] or p2[\"x\"] < p3[\"x\"] or p1[\"x\"] > p4[\"x\"])\n",
    "\n",
    "    \n",
    "    def verify_overlap(self, word, position): # true if overlaps, false if not\n",
    "        new_rect = {\n",
    "            \"x\": position[\"x\"],\n",
    "            \"y\": position[\"y\"],\n",
    "            \"width\": word.width,\n",
    "            \"height\": word.height\n",
    "        }\n",
    "        for filled_rect in self.canvas:\n",
    "            if self.rect_intersection(filled_rect, new_rect):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "\n",
    "    def spiral(self): # returns an [] with positions to test \n",
    "        start_point = {\"x\": 0, \"y\": 0}\n",
    "        start_point[\"x\"] = random.randint(int(self.canvas_size[\"x\"]*0.4), int(self.canvas_size[\"x\"]*0.5))\n",
    "        start_point[\"y\"] = random.randint(int(self.canvas_size[\"y\"]*0.4), int(self.canvas_size[\"y\"]*0.5))\n",
    "        points = [start_point]\n",
    "        # x = (a + b*theta)cos(theta)\n",
    "        # y = (a + b*theta)sin(theta)\n",
    "\n",
    "        # b = a final - a ini / 2 pi n  n=number of turns\n",
    "        a_ini = 0\n",
    "        a_final = min(self.canvas_size[\"y\"]-start_point[\"y\"], start_point[\"y\"])\n",
    "\n",
    "        b = (a_final - a_ini)/(2*3.14159*(self.canvas_size[\"y\"]/10))\n",
    "\n",
    "        thetas = [ (self.canvas_size[\"y\"]/10 * 2)/1000 *x for x in range(1000)]\n",
    "        for i in thetas: #500 points\n",
    "            x = ( a_ini + b*i )*cos(i) + start_point[\"x\"]\n",
    "            y = ( a_ini + b*i )*sin(i) + start_point[\"y\"]\n",
    "            points.append({\"x\": x, \"y\": y})\n",
    "\n",
    "        return points\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1920, 0, 1080]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud = Cloud()\n",
    "\n",
    "words = []\n",
    "for kw, d in kw_max_dim.items():\n",
    "    words.append(Word(kw, {\"width\": d[0], \"height\": d[1]}, kw_fs[kw]))\n",
    "    \n",
    "for w in words:\n",
    "    cloud.add_word(w)\n",
    "\n",
    "#print(cloud.canvas)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "for w in cloud.canvas:\n",
    "    ax.text(w[\"x\"], w[\"y\"], w[\"word\"], fontsize=w[\"font_size\"]//3)\n",
    "\n",
    "ax.axis([0, 1920, 0, 1080])\n",
    "\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('clouds.html', 'w')\n",
    "f.write('<svg viewbox=\"0 0 1920 1080\">')\n",
    "for w in cloud.canvas:\n",
    "    f.write('<text x=\"{}\" y=\"{}\" font-family=\"Verdana\" font-size=\"{}\">'.format(w[\"x\"], w[\"y\"], w[\"font_size\"]))\n",
    "    f.write(w[\"word\"])\n",
    "    f.write('</text>\\n')\n",
    "f.write('</svg>')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Vertex:\n",
    "    def __init__(self, node):\n",
    "        self.id = node # we have a dict {id : { word, weight } }\n",
    "        self.adjacent = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(self.id) + ' adjacent: ' + str([x.id for x in self.adjacent])\n",
    "\n",
    "    def add_neighbor(self, neighbor, weight=0):\n",
    "        self.adjacent[neighbor] = weight\n",
    "\n",
    "    def get_connections(self):\n",
    "        return self.adjacent.keys()  \n",
    "\n",
    "    def get_id(self):\n",
    "        return self.id\n",
    "\n",
    "    def get_weight(self, neighbor):\n",
    "        return self.adjacent[neighbor]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    def __init__(self):\n",
    "        self.vert_dict = {}\n",
    "        self.num_vertices = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.vert_dict.values())\n",
    "\n",
    "    def add_vertex(self, node):\n",
    "        self.num_vertices = self.num_vertices + 1\n",
    "        new_vertex = Vertex(node)\n",
    "        self.vert_dict[node] = new_vertex\n",
    "        return new_vertex\n",
    "\n",
    "    def get_vertex(self, n):\n",
    "        if n in self.vert_dict:\n",
    "            return self.vert_dict[n]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def add_edge(self, frm, to, cost = 0):\n",
    "        if frm not in self.vert_dict:\n",
    "            self.add_vertex(frm)\n",
    "        if to not in self.vert_dict:\n",
    "            self.add_vertex(to)\n",
    "\n",
    "        self.vert_dict[frm].add_neighbor(self.vert_dict[to], cost)\n",
    "        self.vert_dict[to].add_neighbor(self.vert_dict[frm], cost)\n",
    "\n",
    "    def get_vertices(self):\n",
    "        return self.vert_dict.keys()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Star Forest Clustering and putting together\n",
    "'''\n",
    "\n",
    "def StarForestAlgo(g):\n",
    "    '''\n",
    "    g = similarity graph\n",
    "    '''\n",
    "    stars = []\n",
    "    while True:\n",
    "        usedVertices = []\n",
    "        bestCenter = findBestCenter(g, usedVertices)\n",
    "\n",
    "        if bestCenter is None:\n",
    "            break; \n",
    "        \n",
    "        star, usedVertices = createGraphStar(g, bestCenter, usedVertices) # graph, vertice, [vertices]\n",
    "        print(usedVertices)\n",
    "        stars.append(star)\n",
    "        \n",
    "        \n",
    "    return stars\n",
    "\n",
    "\n",
    "def findBestCenter(g, usedVertices): # graph, [vertices]\n",
    "    best_sum = 0\n",
    "    best_center = None\n",
    "    for v in g.get_vertices():\n",
    "        if v not in usedVertices:\n",
    "            sum = getSumOfConnectedEdges(g, v, usedVertices)\n",
    "            if sum > best_sum:\n",
    "                best_center = v\n",
    "    return best_center\n",
    "\n",
    "\n",
    "def getSumOfConnectedEdges(g, v, usedVertices):\n",
    "    sum = 0\n",
    "    connections = g.get_vertex(v).get_connections()\n",
    "    for c in connections:\n",
    "        if c not in usedVertices:\n",
    "            sum += g.get_vertex(v).get_weight(c)\n",
    "    return sum\n",
    "    \n",
    "\n",
    "def createGraphStar(g, bestCenter, usedVertices):\n",
    "    star = Graph()\n",
    "    for v in g.get_vertex(bestCenter).get_connections():\n",
    "        if v not in usedVertices and g.get_vertex(bestCenter) != v:\n",
    "            star.add_edge(bestCenter, v, g.get_vertex(bestCenter).get_weight(v))\n",
    "            print(v)\n",
    "            usedVertices.append(v)\n",
    "    return star, usedVertices\n",
    "\n",
    "\n",
    "\n",
    "g = Graph()\n",
    "\n",
    "g.add_vertex('a')\n",
    "g.add_vertex('b')\n",
    "g.add_vertex('c')\n",
    "g.add_vertex('d')\n",
    "g.add_vertex('e')\n",
    "g.add_vertex('f')\n",
    "\n",
    "g.add_edge('a', 'b', 7)  \n",
    "g.add_edge('a', 'c', 9)\n",
    "g.add_edge('a', 'f', 14)\n",
    "g.add_edge('b', 'c', 10)\n",
    "g.add_edge('b', 'd', 15)\n",
    "g.add_edge('c', 'd', 11)\n",
    "g.add_edge('c', 'f', 2)\n",
    "g.add_edge('d', 'e', 6)\n",
    "g.add_edge('e', 'f', 9)\n",
    "\n",
    "#StarForestAlgo(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aspect ration of words : font_size (length) font_size*0.7(width)\n",
    "# Aspect ration of SVG file is 16:9\n",
    "\n",
    "# How to draw V1 \n",
    "# Create a polygon with the number of vertices = number of clusters \n",
    "# here cluster size = 3 so a triangle (not ever going to exceed 5)\n",
    "# 3 rectangles to fit within the first rectangle \n",
    "\n",
    "# in a 16:9\n",
    "\n",
    "# Cluster one in rect 1 (y = 16, 9/4) (w: 8, l: 9/2)\n",
    "# cluster Two in rect 2 (y = 16, 9/4*3) (w: 8, l: 9/2)\n",
    "# Cluster three in rect 3 (y = 8, 9/4) Biggest cluster ? (w: 8, l: 9/2)\n",
    "\n",
    "# Where to put the words \n",
    "# Start with the highest frequence with the biggest font : assign max font size before starting to draw\n",
    "# If the next one is smaller in frequence, font size drops by \n",
    "# font size 35 to 18\n",
    "# random choice where the word fits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
